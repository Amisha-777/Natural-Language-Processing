# 📘 Machine Learning Classification & Text Classification 

## 📌 Overview
This document covers **text classification**, **supervised and unsupervised learning**, **classification techniques**, **decision trees**, and **evaluation metrics** like **entropy and information gain**.

---

## 🏷️ **1. Supervised Learning**

**Definition:** A model is trained using labeled data (each input has a corresponding output).
**Goal:** Predict correct labels for new inputs based on learned patterns.

### 🔹 **Examples:**
- **Regression:** Predicting house prices based on features (size, location).
- **Classification:** Categorizing emails as spam or not spam.

### 🔹 **Characteristics:**
- Requires labeled data.
- Involves a training phase (error minimization via techniques like gradient descent).
- Produces categories (classification) or continuous values (regression).

---

## 🏷️ **2. Unsupervised Learning**

**Definition:** The model explores patterns in unlabeled data without predefined categories.
**Goal:** Find hidden structures or insights from raw data.

### 🔹 **Examples:**
- **Clustering:** Segmenting customers based on purchasing behavior.
- **Dimensionality Reduction:** Principal Component Analysis (PCA) for visualizing high-dimensional data.

### 🔹 **Characteristics:**
- No labeled data required.
- Focuses on pattern discovery.
- Includes clustering, association rules, and dimensionality reduction.

---

## 🏷️ **3. Classification Process**
A classification model follows these steps:
1. **Data Collection** – Gather labeled examples.
2. **Data Preprocessing** – Clean, normalize, and format data.
3. **Feature Extraction** – Convert raw text into numerical vectors (TF-IDF, word embeddings).
4. **Model Training** – Use supervised learning techniques to train the model.
5. **Model Evaluation** – Measure accuracy and improve performance.
6. **Prediction** – Classify new data using the trained model.

---

## 🏷️ **4. Classification Techniques & Use Cases**

| Algorithm | Use Case | Description |
|-----------|---------|-------------|
| **Logistic Regression** | Binary classification (Spam vs. Not Spam) | Uses a sigmoid function to predict probabilities and is effective for linearly separable data. |
| **Multinomial Naïve Bayes** | Text classification (Sentiment Analysis) | Probabilistic classifier based on Bayes' theorem, best suited for text data. |
| **Decision Trees** | Customer segmentation | Recursive partitioning method that splits data based on attribute values. |
| **Support Vector Machines (SVM)** | High-dimensional classification | Finds the optimal hyperplane that best separates different classes. |
| **K-Nearest Neighbors (KNN)** | Pattern recognition | Classifies data based on the majority vote of its nearest neighbors. |
| **Random Forest** | Fraud detection  | Ensemble method using multiple decision trees to improve accuracy and reduce overfitting. |
| **Artificial Neural Networks (ANNs)** | Deep learning applications  | Uses interconnected neurons to recognize complex patterns in data. |
| **Gradient Boosting (XGBoost, LightGBM)** | Predictive modeling  | Builds models sequentially, correcting previous errors to improve predictions. |

---

## 🌳 **5. Decision Trees**
A decision tree classifies data by splitting it based on attributes.

### 🔹 **Selecting an Attribute for a Node**
- Partition data points into subgroups to maximize homogeneity.
- Use **Information Gain (IG)** to determine the best split.

### 🔹 **Entropy (H) & Example**
- Measures disorder in a dataset.
- Formula:
H(S) = - Σ [ pᵢ * log₂(pᵢ) ]

where:
- pᵢ is the proportion of examples belonging to class i.

 
Example: If a dataset has **16 good credit** and **14 bad credit** customers:
\[
H(S) = - (16/30) \log_2(16/30) - (14/30) \log_2(14/30)
\]

### 🔹 **Information Gain (IG)**
- Reduction in entropy after splitting data.
- Formula:
IG(S, A) = H(S) - Σ ( |Sᵥ| / |S| * H(Sᵥ) )

where:
H(S) is the entropy of the original dataset.
Sᵥ represents subsets created by splitting on attribute A.
|Sᵥ| / |S| is the proportion of examples in subset Sᵥ.

#### **Splitting by Balance vs. Residence**
| Criteria | Entropy Reduction | Explanation |
|----------|------------------|-------------|
| **Balance** | High | More decisive classification |
| **Residence** | Medium | Less distinct separation |

Use Case: **Balance** is a better split when predicting credit risk, while **Residence** may be relevant for demographic-based segmentation.

---

## 🚀 **6. Steps to Apply Text Classification**
1. **Text Data** – Example text documents (e.g., sentences related to Python and Java). Each document is assigned a label (**1 for Python-related, 0 for Java-related**).
2. **TF-IDF Vectorization** – Convert text documents into numerical features using `TfidfVectorizer`. The `stop_words='english'` argument removes common stopwords.
3. **Train-Test Split** – Data is split into **70% training** and **30% testing** using `train_test_split`.
4. **Training** – `DecisionTreeClassifier` from `scikit-learn` is used to train the model on the vectorized text data.
5. **Prediction** – The trained model is used to predict labels for new text (e.g., `"I am learning Java for enterprise applications"`).

---

## 📝 **7. Key Takeaways**
✅ **Text classification** is used in spam detection, sentiment analysis, and customer segmentation.  
✅ **Decision trees** classify data by recursively splitting it using IG.  
✅ **Entropy** measures disorder; lower entropy means better classification.  
✅ **Information Gain (IG)** selects the best attribute for splitting.  

---
