{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fbcc9364-dd01-4d3e-806b-7b2d7933fc9a",
   "metadata": {},
   "source": [
    "# Natural Language Processing with spaCy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a271cce8-a911-4d81-a81f-24b0e0ab7ee2",
   "metadata": {},
   "source": [
    "#### A modern, high-performance NLP library with an advanced machine learning-based pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8ced8636-6c8e-4236-b050-c309450430bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing necessary library and loading the language model\n",
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87ce5ab6-59a2-4fce-b073-03a01a4b5d9a",
   "metadata": {},
   "source": [
    "### Basic Usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "746cf114-0f31-472a-9dbc-f24dd1b3d4c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spaCy NUM nsubj\n",
      "is AUX ROOT\n",
      "an DET det\n",
      "amazing ADJ amod\n",
      "NLP PROPN compound\n",
      "library NOUN attr\n",
      "for ADP prep\n",
      "Python PROPN pobj\n",
      "! PUNCT punct\n"
     ]
    }
   ],
   "source": [
    "# Process Text\n",
    "doc = nlp(\"spaCy is an amazing NLP library for Python!\")\n",
    "\n",
    "# Access tokens\n",
    "for token in doc:\n",
    "    print(token.text, token.pos_, token.dep_)\n",
    "\n",
    "# token.text: The tokenized word.  \n",
    "# token.pos_: Part-of-speech tagging\n",
    "# token.dep_: Dependency relation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56ef1060-dff4-47a0-a208-c7c5a60c0b78",
   "metadata": {},
   "source": [
    "## 1. Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "10b27633-04b5-41cf-945c-3ac5ec7c8baa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The = the\n",
      "cars = car\n",
      "are = be\n",
      "being = be\n",
      "driven = drive\n",
      "carefully = carefully\n",
      "while = while\n",
      "the = the\n",
      "dogs = dog\n",
      "are = be\n",
      "barking = bark\n",
      ". = .\n"
     ]
    }
   ],
   "source": [
    "# reduces words to their base or dictionary form\n",
    "\n",
    "doc = nlp(\"The cars are being driven carefully while the dogs are barking.\")\n",
    "for token in doc:\n",
    "    print(f\"{token.text} = {token.lemma_}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fcf33d9-64d1-4be2-8195-43adfdc966c2",
   "metadata": {},
   "source": [
    "## 2. Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9365e545-d609-4ff0-8a47-a68b741fc1f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I\n",
      "am\n",
      "learning\n",
      "natural\n",
      "language\n",
      "processing\n",
      "using\n",
      "spaCy\n",
      ".\n",
      "Number of tokens: 9\n"
     ]
    }
   ],
   "source": [
    "# splits text into words, punctuation and other meaningful elements\n",
    "\n",
    "doc = nlp(\"I am learning natural language processing using spaCy.\")\n",
    "for token in doc:\n",
    "    print(token.text)\n",
    "print(\"Number of tokens:\", len(doc))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c08a643-9dcc-4a7e-b2bd-913667c6db3f",
   "metadata": {},
   "source": [
    "## 3. Named Entity Recognition (NER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c12cb409-8b7b-4446-b09c-858f912818c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entity: Barack Obama, Label: PERSON\n",
      "Entity: 44th, Label: ORDINAL\n",
      "Entity: the United States, Label: GPE\n",
      "Entity: Hawaii, Label: GPE\n"
     ]
    }
   ],
   "source": [
    "# identifies named entities like people, organizations, and locations in text. \n",
    "\n",
    "doc = nlp(\"Barack Obama served as the 44th president of the United States and was born in Hawaii.\")\n",
    "for ent in doc.ents:\n",
    "    print(f\"Entity: {ent.text}, Label: {ent.label_}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae4f8c2f-a5e6-4a81-a158-ca5abc7de36a",
   "metadata": {},
   "source": [
    "## 4. Sentence Segmentation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5553ba74-44a3-45da-86fa-619998ecf59f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Machine learning is fascinating.\n",
      "It enables to learn from the data.\n",
      "Natural language processing is a subfield of AI.\n"
     ]
    }
   ],
   "source": [
    "# splits text into individual sentences.\n",
    "\n",
    "doc = nlp(\"Machine learning is fascinating. It enables to learn from the data. Natural language processing is a subfield of AI.\")\n",
    "for sent in doc.sents:\n",
    "    print(sent.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ea1d04e-aa62-47ba-9fce-f9cb9e98d9cd",
   "metadata": {},
   "source": [
    "## 5. Stop Word Removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2a749554-cf4b-4f14-98fb-7e4fba2fadd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Text: Stop words are common words that often do not add much meaning.\n",
      "Filtered Tokens: Stop words common words add meaning .\n"
     ]
    }
   ],
   "source": [
    "# eliminates common words (and, the) that do not add meaning\n",
    "\n",
    "text = \"Stop words are common words that often do not add much meaning.\"\n",
    "doc = nlp(text)\n",
    "filtered_tokens = [token.text for token in doc if not token.is_stop]\n",
    "print(\"Original Text:\", text)\n",
    "print(\"Filtered Tokens:\", \" \".join(filtered_tokens))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6085c5c-de0a-4edb-8a74-7985686c4bd0",
   "metadata": {},
   "source": [
    "## 6. Parts of Speech (POS) Recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "761e12b8-ebbe-4ce3-be69-47f8cb482eb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token           POS        Explanation\n",
      "----------------------------------------\n",
      "Natural         ADJ        adjective\n",
      "language        NOUN       noun\n",
      "processing      NOUN       noun\n",
      "is              AUX        auxiliary\n",
      "an              DET        determiner\n",
      "exciting        ADJ        adjective\n",
      "field           NOUN       noun\n",
      "of              ADP        adposition\n",
      "artificial      ADJ        adjective\n",
      "intelligence    NOUN       noun\n",
      ".               PUNCT      punctuation\n"
     ]
    }
   ],
   "source": [
    "# identifies the grammatical role of each word in the text\n",
    "\n",
    "text = \"Natural language processing is an exciting field of artificial intelligence.\"\n",
    "doc = nlp(text)\n",
    "\n",
    "#POS Tagging\n",
    "print(f\"{'Token':<15} {'POS':<10} {'Explanation'}\")\n",
    "print(\"-\"*40)\n",
    "for token in doc:\n",
    "    print(f\"{token.text:<15} {token.pos_:<10} {spacy.explain(token.pos_)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee9e0884-51e3-45b6-a312-57adc1f83f86",
   "metadata": {},
   "source": [
    "# Lab Wrap-Up Exercise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bab0038d-fd71-4908-86b1-9a7e0bccb50e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sentence Segmentation:\n",
      "Sentence 1: Albert Einstein, a theoretical physicist, was born in Germany.\n",
      "Sentence 2: He developed the theory of relativity, which is one ofthe two pillars of modern physics.\n",
      "\n",
      "Tokenization:\n",
      "Albert\n",
      "Einstein\n",
      ",\n",
      "a\n",
      "theoretical\n",
      "physicist\n",
      ",\n",
      "was\n",
      "born\n",
      "in\n",
      "Germany\n",
      ".\n",
      "He\n",
      "developed\n",
      "the\n",
      "theory\n",
      "of\n",
      "relativity\n",
      ",\n",
      "which\n",
      "is\n",
      "one\n",
      "ofthe\n",
      "two\n",
      "pillars\n",
      "of\n",
      "modern\n",
      "physics\n",
      ".\n",
      "Number of tokens: 29\n",
      "\n",
      "Stop Word Removal:\n",
      "Original Text: Albert Einstein, a theoretical physicist, was born in Germany. He developed the theory of relativity, which is one ofthe two pillars of modern physics.\n",
      "Filtered Tokens: ['Albert', 'Einstein', ',', 'theoretical', 'physicist', ',', 'born', 'Germany', '.', 'developed', 'theory', 'relativity', ',', 'ofthe', 'pillars', 'modern', 'physics', '.']\n",
      "\n",
      "Lemmatization:\n",
      "Albert = Albert\n",
      "Einstein = Einstein\n",
      ", = ,\n",
      "a = a\n",
      "theoretical = theoretical\n",
      "physicist = physicist\n",
      ", = ,\n",
      "was = be\n",
      "born = bear\n",
      "in = in\n",
      "Germany = Germany\n",
      ". = .\n",
      "He = he\n",
      "developed = develop\n",
      "the = the\n",
      "theory = theory\n",
      "of = of\n",
      "relativity = relativity\n",
      ", = ,\n",
      "which = which\n",
      "is = be\n",
      "one = one\n",
      "ofthe = ofthe\n",
      "two = two\n",
      "pillars = pillar\n",
      "of = of\n",
      "modern = modern\n",
      "physics = physic\n",
      ". = .\n",
      "\n",
      "POS Tagging:\n",
      "Token           POS       \n",
      "--------------------\n",
      "Albert          PROPN     \n",
      "Einstein        PROPN     \n",
      ",               PUNCT     \n",
      "a               DET       \n",
      "theoretical     ADJ       \n",
      "physicist       NOUN      \n",
      ",               PUNCT     \n",
      "was             AUX       \n",
      "born            VERB      \n",
      "in              ADP       \n",
      "Germany         PROPN     \n",
      ".               PUNCT     \n",
      "He              PRON      \n",
      "developed       VERB      \n",
      "the             DET       \n",
      "theory          NOUN      \n",
      "of              ADP       \n",
      "relativity      NOUN      \n",
      ",               PUNCT     \n",
      "which           PRON      \n",
      "is              AUX       \n",
      "one             NUM       \n",
      "ofthe           NOUN      \n",
      "two             NUM       \n",
      "pillars         NOUN      \n",
      "of              ADP       \n",
      "modern          ADJ       \n",
      "physics         NOUN      \n",
      ".               PUNCT     \n",
      "\n",
      "Named Entity Recognition:\n",
      "Entity: Albert Einstein, Label: PERSON\n",
      "Entity: Germany, Label: GPE\n",
      "Entity: one, Label: CARDINAL\n",
      "Entity: two, Label: CARDINAL\n"
     ]
    }
   ],
   "source": [
    "text = \"Albert Einstein, a theoretical physicist, was born in Germany. He developed the theory of relativity, which is one ofthe two pillars of modern physics.\"\n",
    "\n",
    "# Processing the text\n",
    "doc = nlp(text)\n",
    "\n",
    "# Sentence Segmentation\n",
    "print(\"\\nSentence Segmentation:\")\n",
    "for i, sent in enumerate(doc.sents, 1):\n",
    "    print(f\"Sentence {i}: {sent.text}\")\n",
    "\n",
    "# Tokenization\n",
    "print(\"\\nTokenization:\")\n",
    "for token in doc:\n",
    "    print(token.text)\n",
    "print(\"Number of tokens:\", len(doc))\n",
    "\n",
    "# Stop word Removal\n",
    "print(\"\\nStop Word Removal:\")\n",
    "filtered_tokens = [token.text for token in doc if not token.is_stop]\n",
    "print(\"Original Text:\", text)\n",
    "print(\"Filtered Tokens:\", filtered_tokens)\n",
    "\n",
    "# Lemmatization\n",
    "print(\"\\nLemmatization:\")\n",
    "for token in doc:\n",
    "    print(f\"{token.text} = {token.lemma_}\")\n",
    "\n",
    "# POS Tagging\n",
    "print(\"\\nPOS Tagging:\")\n",
    "print(f\"{'Token':<15} {'POS':<10}\")\n",
    "print(\"-\"*20)\n",
    "for token in doc:\n",
    "    print(f\"{token.text:<15} {token.pos_:<10}\")\n",
    "\n",
    "# Named Entity Recognition (NER)\n",
    "print(\"\\nNamed Entity Recognition:\")\n",
    "for ent in doc.ents:\n",
    "    print(f\"Entity: {ent.text}, Label: {ent.label_}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
