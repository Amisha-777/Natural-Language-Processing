{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b52a0bde-94bd-4ace-bb7f-f6ca8e013e0a",
   "metadata": {},
   "source": [
    "# Natural Language Processing with NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0d4bd946-cdde-42a0-88f8-fe13f42996c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading collection 'all'\n",
      "[nltk_data]    | \n",
      "[nltk_data]    | Downloading package abc to\n",
      "[nltk_data]    |     C:\\Users\\hp\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package abc is already up-to-date!\n",
      "[nltk_data]    | Downloading package alpino to\n",
      "[nltk_data]    |     C:\\Users\\hp\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package alpino is already up-to-date!\n",
      "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]    |     C:\\Users\\hp\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package averaged_perceptron_tagger is already up-\n",
      "[nltk_data]    |       to-date!\n",
      "[nltk_data]    | Downloading package averaged_perceptron_tagger_eng to\n",
      "[nltk_data]    |     C:\\Users\\hp\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package averaged_perceptron_tagger_eng is already\n",
      "[nltk_data]    |       up-to-date!\n",
      "[nltk_data]    | Downloading package averaged_perceptron_tagger_ru to\n",
      "[nltk_data]    |     C:\\Users\\hp\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package averaged_perceptron_tagger_ru is already\n",
      "[nltk_data]    |       up-to-date!\n",
      "[nltk_data]    | Downloading package averaged_perceptron_tagger_rus to\n",
      "[nltk_data]    |     C:\\Users\\hp\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package averaged_perceptron_tagger_rus is already\n",
      "[nltk_data]    |       up-to-date!\n",
      "[nltk_data]    | Downloading package basque_grammars to\n",
      "[nltk_data]    |     C:\\Users\\hp\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package basque_grammars is already up-to-date!\n",
      "[nltk_data]    | Downloading package bcp47 to\n",
      "[nltk_data]    |     C:\\Users\\hp\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package bcp47 is already up-to-date!\n",
      "[nltk_data]    | Downloading package biocreative_ppi to\n",
      "[nltk_data]    |     C:\\Users\\hp\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package biocreative_ppi is already up-to-date!\n",
      "[nltk_data]    | Downloading package bllip_wsj_no_aux to\n",
      "[nltk_data]    |     C:\\Users\\hp\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package bllip_wsj_no_aux is already up-to-date!\n",
      "[nltk_data]    | Downloading package book_grammars to\n",
      "[nltk_data]    |     C:\\Users\\hp\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package book_grammars is already up-to-date!\n",
      "[nltk_data]    | Downloading package brown to\n",
      "[nltk_data]    |     C:\\Users\\hp\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package brown is already up-to-date!\n",
      "[nltk_data]    | Downloading package brown_tei to\n",
      "[nltk_data]    |     C:\\Users\\hp\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package brown_tei is already up-to-date!\n",
      "[nltk_data]    | Downloading package cess_cat to\n",
      "[nltk_data]    |     C:\\Users\\hp\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package cess_cat is already up-to-date!\n",
      "[nltk_data]    | Downloading package cess_esp to\n",
      "[nltk_data]    |     C:\\Users\\hp\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package cess_esp is already up-to-date!\n",
      "[nltk_data]    | Downloading package chat80 to\n",
      "[nltk_data]    |     C:\\Users\\hp\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package chat80 is already up-to-date!\n",
      "[nltk_data]    | Downloading package city_database to\n",
      "[nltk_data]    |     C:\\Users\\hp\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package city_database is already up-to-date!\n",
      "[nltk_data]    | Downloading package cmudict to\n",
      "[nltk_data]    |     C:\\Users\\hp\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package cmudict is already up-to-date!\n",
      "[nltk_data]    | Downloading package comparative_sentences to\n",
      "[nltk_data]    |     C:\\Users\\hp\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package comparative_sentences is already up-to-\n",
      "[nltk_data]    |       date!\n",
      "[nltk_data]    | Downloading package comtrans to\n",
      "[nltk_data]    |     C:\\Users\\hp\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package comtrans is already up-to-date!\n",
      "[nltk_data]    | Downloading package conll2000 to\n",
      "[nltk_data]    |     C:\\Users\\hp\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package conll2000 is already up-to-date!\n",
      "[nltk_data]    | Downloading package conll2002 to\n",
      "[nltk_data]    |     C:\\Users\\hp\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package conll2002 is already up-to-date!\n",
      "[nltk_data]    | Downloading package conll2007 to\n",
      "[nltk_data]    |     C:\\Users\\hp\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package conll2007 is already up-to-date!\n",
      "[nltk_data]    | Downloading package crubadan to\n",
      "[nltk_data]    |     C:\\Users\\hp\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package crubadan is already up-to-date!\n",
      "[nltk_data]    | Downloading package dependency_treebank to\n",
      "[nltk_data]    |     C:\\Users\\hp\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package dependency_treebank is already up-to-date!\n",
      "[nltk_data]    | Downloading package dolch to\n",
      "[nltk_data]    |     C:\\Users\\hp\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package dolch is already up-to-date!\n",
      "[nltk_data]    | Downloading package europarl_raw to\n",
      "[nltk_data]    |     C:\\Users\\hp\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package europarl_raw is already up-to-date!\n",
      "[nltk_data]    | Downloading package extended_omw to\n",
      "[nltk_data]    |     C:\\Users\\hp\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package extended_omw is already up-to-date!\n",
      "[nltk_data]    | Downloading package floresta to\n",
      "[nltk_data]    |     C:\\Users\\hp\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package floresta is already up-to-date!\n",
      "[nltk_data]    | Downloading package framenet_v15 to\n",
      "[nltk_data]    |     C:\\Users\\hp\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package framenet_v15 is already up-to-date!\n",
      "[nltk_data]    | Downloading package framenet_v17 to\n",
      "[nltk_data]    |     C:\\Users\\hp\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package framenet_v17 is already up-to-date!\n",
      "[nltk_data]    | Downloading package gazetteers to\n",
      "[nltk_data]    |     C:\\Users\\hp\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package gazetteers is already up-to-date!\n",
      "[nltk_data]    | Downloading package genesis to\n",
      "[nltk_data]    |     C:\\Users\\hp\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package genesis is already up-to-date!\n",
      "[nltk_data]    | Downloading package gutenberg to\n",
      "[nltk_data]    |     C:\\Users\\hp\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package gutenberg is already up-to-date!\n",
      "[nltk_data]    | Downloading package ieer to\n",
      "[nltk_data]    |     C:\\Users\\hp\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package ieer is already up-to-date!\n",
      "[nltk_data]    | Downloading package inaugural to\n",
      "[nltk_data]    |     C:\\Users\\hp\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package inaugural is already up-to-date!\n",
      "[nltk_data]    | Downloading package indian to\n",
      "[nltk_data]    |     C:\\Users\\hp\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package indian is already up-to-date!\n",
      "[nltk_data]    | Downloading package jeita to\n",
      "[nltk_data]    |     C:\\Users\\hp\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package jeita is already up-to-date!\n",
      "[nltk_data]    | Downloading package kimmo to\n",
      "[nltk_data]    |     C:\\Users\\hp\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package kimmo is already up-to-date!\n",
      "[nltk_data]    | Downloading package knbc to\n",
      "[nltk_data]    |     C:\\Users\\hp\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package knbc is already up-to-date!\n",
      "[nltk_data]    | Downloading package large_grammars to\n",
      "[nltk_data]    |     C:\\Users\\hp\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package large_grammars is already up-to-date!\n",
      "[nltk_data]    | Downloading package lin_thesaurus to\n",
      "[nltk_data]    |     C:\\Users\\hp\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package lin_thesaurus is already up-to-date!\n",
      "[nltk_data]    | Downloading package mac_morpho to\n",
      "[nltk_data]    |     C:\\Users\\hp\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package mac_morpho is already up-to-date!\n",
      "[nltk_data]    | Downloading package machado to\n",
      "[nltk_data]    |     C:\\Users\\hp\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package machado is already up-to-date!\n",
      "[nltk_data]    | Downloading package masc_tagged to\n",
      "[nltk_data]    |     C:\\Users\\hp\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package masc_tagged is already up-to-date!\n",
      "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
      "[nltk_data]    |     C:\\Users\\hp\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data]    | Downloading package maxent_ne_chunker_tab to\n",
      "[nltk_data]    |     C:\\Users\\hp\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package maxent_ne_chunker_tab is already up-to-\n",
      "[nltk_data]    |       date!\n",
      "[nltk_data]    | Downloading package maxent_treebank_pos_tagger to\n",
      "[nltk_data]    |     C:\\Users\\hp\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package maxent_treebank_pos_tagger is already up-\n",
      "[nltk_data]    |       to-date!\n",
      "[nltk_data]    | Downloading package maxent_treebank_pos_tagger_tab to\n",
      "[nltk_data]    |     C:\\Users\\hp\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package maxent_treebank_pos_tagger_tab is already\n",
      "[nltk_data]    |       up-to-date!\n",
      "[nltk_data]    | Downloading package moses_sample to\n",
      "[nltk_data]    |     C:\\Users\\hp\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package moses_sample is already up-to-date!\n",
      "[nltk_data]    | Downloading package movie_reviews to\n",
      "[nltk_data]    |     C:\\Users\\hp\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package movie_reviews is already up-to-date!\n",
      "[nltk_data]    | Downloading package mte_teip5 to\n",
      "[nltk_data]    |     C:\\Users\\hp\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package mte_teip5 is already up-to-date!\n",
      "[nltk_data]    | Downloading package mwa_ppdb to\n",
      "[nltk_data]    |     C:\\Users\\hp\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package mwa_ppdb is already up-to-date!\n",
      "[nltk_data]    | Downloading package names to\n",
      "[nltk_data]    |     C:\\Users\\hp\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package names is already up-to-date!\n",
      "[nltk_data]    | Downloading package nombank.1.0 to\n",
      "[nltk_data]    |     C:\\Users\\hp\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package nombank.1.0 is already up-to-date!\n",
      "[nltk_data]    | Downloading package nonbreaking_prefixes to\n",
      "[nltk_data]    |     C:\\Users\\hp\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package nonbreaking_prefixes is already up-to-date!\n",
      "[nltk_data]    | Downloading package nps_chat to\n",
      "[nltk_data]    |     C:\\Users\\hp\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package nps_chat is already up-to-date!\n",
      "[nltk_data]    | Downloading package omw to\n",
      "[nltk_data]    |     C:\\Users\\hp\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package omw is already up-to-date!\n",
      "[nltk_data]    | Downloading package omw-1.4 to\n",
      "[nltk_data]    |     C:\\Users\\hp\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data]    | Downloading package opinion_lexicon to\n",
      "[nltk_data]    |     C:\\Users\\hp\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package opinion_lexicon is already up-to-date!\n",
      "[nltk_data]    | Downloading package panlex_swadesh to\n",
      "[nltk_data]    |     C:\\Users\\hp\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package panlex_swadesh is already up-to-date!\n",
      "[nltk_data]    | Downloading package paradigms to\n",
      "[nltk_data]    |     C:\\Users\\hp\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package paradigms is already up-to-date!\n",
      "[nltk_data]    | Downloading package pe08 to\n",
      "[nltk_data]    |     C:\\Users\\hp\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package pe08 is already up-to-date!\n",
      "[nltk_data]    | Downloading package perluniprops to\n",
      "[nltk_data]    |     C:\\Users\\hp\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package perluniprops is already up-to-date!\n",
      "[nltk_data]    | Downloading package pil to\n",
      "[nltk_data]    |     C:\\Users\\hp\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package pil is already up-to-date!\n",
      "[nltk_data]    | Downloading package pl196x to\n",
      "[nltk_data]    |     C:\\Users\\hp\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package pl196x is already up-to-date!\n",
      "[nltk_data]    | Downloading package porter_test to\n",
      "[nltk_data]    |     C:\\Users\\hp\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package porter_test is already up-to-date!\n",
      "[nltk_data]    | Downloading package ppattach to\n",
      "[nltk_data]    |     C:\\Users\\hp\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package ppattach is already up-to-date!\n",
      "[nltk_data]    | Downloading package problem_reports to\n",
      "[nltk_data]    |     C:\\Users\\hp\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package problem_reports is already up-to-date!\n",
      "[nltk_data]    | Downloading package product_reviews_1 to\n",
      "[nltk_data]    |     C:\\Users\\hp\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package product_reviews_1 is already up-to-date!\n",
      "[nltk_data]    | Downloading package product_reviews_2 to\n",
      "[nltk_data]    |     C:\\Users\\hp\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package product_reviews_2 is already up-to-date!\n",
      "[nltk_data]    | Downloading package propbank to\n",
      "[nltk_data]    |     C:\\Users\\hp\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package propbank is already up-to-date!\n",
      "[nltk_data]    | Downloading package pros_cons to\n",
      "[nltk_data]    |     C:\\Users\\hp\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package pros_cons is already up-to-date!\n",
      "[nltk_data]    | Downloading package ptb to\n",
      "[nltk_data]    |     C:\\Users\\hp\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package ptb is already up-to-date!\n",
      "[nltk_data]    | Downloading package punkt to\n",
      "[nltk_data]    |     C:\\Users\\hp\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package punkt is already up-to-date!\n",
      "[nltk_data]    | Downloading package punkt_tab to\n",
      "[nltk_data]    |     C:\\Users\\hp\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package punkt_tab is already up-to-date!\n",
      "[nltk_data]    | Downloading package qc to\n",
      "[nltk_data]    |     C:\\Users\\hp\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package qc is already up-to-date!\n",
      "[nltk_data]    | Downloading package reuters to\n",
      "[nltk_data]    |     C:\\Users\\hp\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package reuters is already up-to-date!\n",
      "[nltk_data]    | Downloading package rslp to\n",
      "[nltk_data]    |     C:\\Users\\hp\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package rslp is already up-to-date!\n",
      "[nltk_data]    | Downloading package rte to\n",
      "[nltk_data]    |     C:\\Users\\hp\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package rte is already up-to-date!\n",
      "[nltk_data]    | Downloading package sample_grammars to\n",
      "[nltk_data]    |     C:\\Users\\hp\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package sample_grammars is already up-to-date!\n",
      "[nltk_data]    | Downloading package semcor to\n",
      "[nltk_data]    |     C:\\Users\\hp\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package semcor is already up-to-date!\n",
      "[nltk_data]    | Downloading package senseval to\n",
      "[nltk_data]    |     C:\\Users\\hp\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package senseval is already up-to-date!\n",
      "[nltk_data]    | Downloading package sentence_polarity to\n",
      "[nltk_data]    |     C:\\Users\\hp\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package sentence_polarity is already up-to-date!\n",
      "[nltk_data]    | Downloading package sentiwordnet to\n",
      "[nltk_data]    |     C:\\Users\\hp\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package sentiwordnet is already up-to-date!\n",
      "[nltk_data]    | Downloading package shakespeare to\n",
      "[nltk_data]    |     C:\\Users\\hp\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package shakespeare is already up-to-date!\n",
      "[nltk_data]    | Downloading package sinica_treebank to\n",
      "[nltk_data]    |     C:\\Users\\hp\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package sinica_treebank is already up-to-date!\n",
      "[nltk_data]    | Downloading package smultron to\n",
      "[nltk_data]    |     C:\\Users\\hp\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package smultron is already up-to-date!\n",
      "[nltk_data]    | Downloading package snowball_data to\n",
      "[nltk_data]    |     C:\\Users\\hp\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package snowball_data is already up-to-date!\n",
      "[nltk_data]    | Downloading package spanish_grammars to\n",
      "[nltk_data]    |     C:\\Users\\hp\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package spanish_grammars is already up-to-date!\n",
      "[nltk_data]    | Downloading package state_union to\n",
      "[nltk_data]    |     C:\\Users\\hp\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package state_union is already up-to-date!\n",
      "[nltk_data]    | Downloading package stopwords to\n",
      "[nltk_data]    |     C:\\Users\\hp\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package stopwords is already up-to-date!\n",
      "[nltk_data]    | Downloading package subjectivity to\n",
      "[nltk_data]    |     C:\\Users\\hp\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package subjectivity is already up-to-date!\n",
      "[nltk_data]    | Downloading package swadesh to\n",
      "[nltk_data]    |     C:\\Users\\hp\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package swadesh is already up-to-date!\n",
      "[nltk_data]    | Downloading package switchboard to\n",
      "[nltk_data]    |     C:\\Users\\hp\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package switchboard is already up-to-date!\n",
      "[nltk_data]    | Downloading package tagsets to\n",
      "[nltk_data]    |     C:\\Users\\hp\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package tagsets is already up-to-date!\n",
      "[nltk_data]    | Downloading package tagsets_json to\n",
      "[nltk_data]    |     C:\\Users\\hp\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package tagsets_json is already up-to-date!\n",
      "[nltk_data]    | Downloading package timit to\n",
      "[nltk_data]    |     C:\\Users\\hp\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package timit is already up-to-date!\n",
      "[nltk_data]    | Downloading package toolbox to\n",
      "[nltk_data]    |     C:\\Users\\hp\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package toolbox is already up-to-date!\n",
      "[nltk_data]    | Downloading package treebank to\n",
      "[nltk_data]    |     C:\\Users\\hp\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package treebank is already up-to-date!\n",
      "[nltk_data]    | Downloading package twitter_samples to\n",
      "[nltk_data]    |     C:\\Users\\hp\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package twitter_samples is already up-to-date!\n",
      "[nltk_data]    | Downloading package udhr to\n",
      "[nltk_data]    |     C:\\Users\\hp\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package udhr is already up-to-date!\n",
      "[nltk_data]    | Downloading package udhr2 to\n",
      "[nltk_data]    |     C:\\Users\\hp\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package udhr2 is already up-to-date!\n",
      "[nltk_data]    | Downloading package unicode_samples to\n",
      "[nltk_data]    |     C:\\Users\\hp\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package unicode_samples is already up-to-date!\n",
      "[nltk_data]    | Downloading package universal_tagset to\n",
      "[nltk_data]    |     C:\\Users\\hp\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package universal_tagset is already up-to-date!\n",
      "[nltk_data]    | Downloading package universal_treebanks_v20 to\n",
      "[nltk_data]    |     C:\\Users\\hp\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package universal_treebanks_v20 is already up-to-\n",
      "[nltk_data]    |       date!\n",
      "[nltk_data]    | Downloading package vader_lexicon to\n",
      "[nltk_data]    |     C:\\Users\\hp\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package vader_lexicon is already up-to-date!\n",
      "[nltk_data]    | Downloading package verbnet to\n",
      "[nltk_data]    |     C:\\Users\\hp\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package verbnet is already up-to-date!\n",
      "[nltk_data]    | Downloading package verbnet3 to\n",
      "[nltk_data]    |     C:\\Users\\hp\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package verbnet3 is already up-to-date!\n",
      "[nltk_data]    | Downloading package webtext to\n",
      "[nltk_data]    |     C:\\Users\\hp\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package webtext is already up-to-date!\n",
      "[nltk_data]    | Downloading package wmt15_eval to\n",
      "[nltk_data]    |     C:\\Users\\hp\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package wmt15_eval is already up-to-date!\n",
      "[nltk_data]    | Downloading package word2vec_sample to\n",
      "[nltk_data]    |     C:\\Users\\hp\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package word2vec_sample is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet to\n",
      "[nltk_data]    |     C:\\Users\\hp\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package wordnet is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet2021 to\n",
      "[nltk_data]    |     C:\\Users\\hp\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package wordnet2021 is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet2022 to\n",
      "[nltk_data]    |     C:\\Users\\hp\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package wordnet2022 is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet31 to\n",
      "[nltk_data]    |     C:\\Users\\hp\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package wordnet31 is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet_ic to\n",
      "[nltk_data]    |     C:\\Users\\hp\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package wordnet_ic is already up-to-date!\n",
      "[nltk_data]    | Downloading package words to\n",
      "[nltk_data]    |     C:\\Users\\hp\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package words is already up-to-date!\n",
      "[nltk_data]    | Downloading package ycoe to\n",
      "[nltk_data]    |     C:\\Users\\hp\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package ycoe is already up-to-date!\n",
      "[nltk_data]    | \n",
      "[nltk_data]  Done downloading collection all\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# importing necessary libraries and packages\n",
    "import nltk\n",
    "nltk.download('all')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2948f4d4-f311-4e62-be5f-7988f481095e",
   "metadata": {},
   "source": [
    "## 1. Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "94dead7b-9f52-4c86-a85c-a65442cb2dd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentences for Text 1: ['Climate change is one of the greatest challenges of our time.', 'The latest report, released on January 10, 2025, states that global temperatures have risen by 1.5Â°C.', 'Rising sea levels are expected to displace over 200 million people by 2050.', 'Experts predict that the cost of addressing these issues could exceed $10 trillion if immediate action isnâ€™t taken.']\n",
      "Sentences for Text 2: ['OMG!!', \"ðŸ˜² Just watched the new netflix documentary... it's mind-blowing!!!\", '#Bingewatching #FutureTech']\n",
      "Sentences for Text 3: ['E = mc^2 is one of the most famous equations in physics.', 'This principle forms the basis of modern theoretical physics.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "text1 = \"Climate change is one of the greatest challenges of our time. The latest report, released on January 10, 2025, states that global temperatures have risen by 1.5Â°C. Rising sea levels are expected to displace over 200 million people by 2050. Experts predict that the cost of addressing these issues could exceed $10 trillion if immediate action isnâ€™t taken.\"\n",
    "text2 = \"OMG!! ðŸ˜² Just watched the new netflix documentary... it's mind-blowing!!! #Bingewatching #FutureTech\"\n",
    "text3 = \"E = mc^2 is one of the most famous equations in physics. This principle forms the basis of modern theoretical physics.\"\n",
    "sentences1 = sent_tokenize(text1)\n",
    "sentences2 = sent_tokenize(text2)\n",
    "sentences3 = sent_tokenize(text3)\n",
    "print(\"Sentences for Text 1:\", sentences1)\n",
    "print(\"Sentences for Text 2:\", sentences2)\n",
    "print(\"Sentences for Text 3:\", sentences3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b64ed0cd-675f-44df-b2cb-96d3291394a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized Words for Text 1: ['Climate', 'change', 'is', 'one', 'of', 'the', 'greatest', 'challenges', 'of', 'our', 'time', '.', 'The', 'latest', 'report', ',', 'released', 'on', 'January', '10', ',', '2025', ',', 'states', 'that', 'global', 'temperatures', 'have', 'risen', 'by', '1.5Â°C', '.', 'Rising', 'sea', 'levels', 'are', 'expected', 'to', 'displace', 'over', '200', 'million', 'people', 'by', '2050', '.', 'Experts', 'predict', 'that', 'the', 'cost', 'of', 'addressing', 'these', 'issues', 'could', 'exceed', '$', '10', 'trillion', 'if', 'immediate', 'action', 'isn', 'â€™', 't', 'taken', '.']\n",
      "Tokenized Words for Text 2: ['OMG', '!', '!', 'ðŸ˜²', 'Just', 'watched', 'the', 'new', 'netflix', 'documentary', '...', 'it', \"'s\", 'mind-blowing', '!', '!', '!', '#', 'Bingewatching', '#', 'FutureTech']\n",
      "Tokenized Words for Text 3: ['E', '=', 'mc^2', 'is', 'one', 'of', 'the', 'most', 'famous', 'equations', 'in', 'physics', '.', 'This', 'principle', 'forms', 'the', 'basis', 'of', 'modern', 'theoretical', 'physics', '.']\n"
     ]
    }
   ],
   "source": [
    "# Word Tokenization: splitting sentences into words\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "words1 = word_tokenize(text1)\n",
    "words2 = word_tokenize(text2)\n",
    "words3 = word_tokenize(text3)\n",
    "print(\"Tokenized Words for Text 1:\", words1)\n",
    "print(\"Tokenized Words for Text 2:\", words2)\n",
    "print(\"Tokenized Words for Text 3:\", words3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0871e34f-dad8-4e4e-b4b0-7810f0c2d4f4",
   "metadata": {},
   "source": [
    "## 2. Stop Words Removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1d35c920-6a45-4887-beb2-1b13af595bae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered words for text 1: ['Climate', 'change', 'one', 'greatest', 'challenges', 'time', '.', 'latest', 'report', ',', 'released', 'January', '10', ',', '2025', ',', 'states', 'global', 'temperatures', 'risen', '1.5Â°C', '.', 'Rising', 'sea', 'levels', 'expected', 'displace', '200', 'million', 'people', '2050', '.', 'Experts', 'predict', 'cost', 'addressing', 'issues', 'could', 'exceed', '$', '10', 'trillion', 'immediate', 'action', 'â€™', 'taken', '.']\n",
      "Filtered words for text 2: ['OMG', '!', '!', 'ðŸ˜²', 'watched', 'new', 'netflix', 'documentary', '...', \"'s\", 'mind-blowing', '!', '!', '!', '#', 'Bingewatching', '#', 'FutureTech']\n",
      "Filtered words for text 3: ['E', '=', 'mc^2', 'one', 'famous', 'equations', 'physics', '.', 'principle', 'forms', 'basis', 'modern', 'theoretical', 'physics', '.']\n"
     ]
    }
   ],
   "source": [
    "# stop words are common words (like the, is, and) that provided little semantic value\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = set(stopwords.words('english'))  # Retrieves a list of common English stop words \n",
    "\n",
    "# processing each word and removing any word that is a stop word\n",
    "filtered_words1 = [word1 for word1 in words1 if word1.lower() not in stop_words] \n",
    "print(\"Filtered words for text 1:\", filtered_words1)\n",
    "\n",
    "filtered_words2 = [word2 for word2 in words2 if word2.lower() not in stop_words] \n",
    "print(\"Filtered words for text 2:\", filtered_words2)\n",
    "\n",
    "filtered_words3 = [word3 for word3 in words3 if word3.lower() not in stop_words] \n",
    "print(\"Filtered words for text 3:\", filtered_words3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7879212-2551-42f5-b11d-b78bbeb33285",
   "metadata": {},
   "source": [
    "## 3. Text Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0be9e624-3b3c-40e1-9fe5-f62a2411586c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalization is reducing words into their base or root form. There are two ways of normalization."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a996347-457e-4c34-b1b3-283eab69d5e6",
   "metadata": {},
   "source": [
    "### 3.1 Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3bebe161-fee7-4c06-b34f-6dba9bfd026d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stemmed Words from text 1: ['climat', 'chang', 'one', 'greatest', 'challeng', 'time', '.', 'latest', 'report', ',', 'releas', 'januari', '10', ',', '2025', ',', 'state', 'global', 'temperatur', 'risen', '1.5Â°c', '.', 'rise', 'sea', 'level', 'expect', 'displac', '200', 'million', 'peopl', '2050', '.', 'expert', 'predict', 'cost', 'address', 'issu', 'could', 'exceed', '$', '10', 'trillion', 'immedi', 'action', 'â€™', 'taken', '.']\n",
      "Stemmed Words from text 2: ['omg', '!', '!', 'ðŸ˜²', 'watch', 'new', 'netflix', 'documentari', '...', \"'s\", 'mind-blow', '!', '!', '!', '#', 'bingewatch', '#', 'futuretech']\n",
      "Stemmed Words from text 3: ['e', '=', 'mc^2', 'one', 'famou', 'equat', 'physic', '.', 'principl', 'form', 'basi', 'modern', 'theoret', 'physic', '.']\n"
     ]
    }
   ],
   "source": [
    "# removing prefixes of suffixes to approximate the root form of the word.\n",
    "\n",
    "from nltk.stem import PorterStemmer\n",
    "ps = PorterStemmer()\n",
    "\n",
    "stemmed_words1 = [ps.stem(word) for word in filtered_words1]\n",
    "print(\"Stemmed Words from text 1:\", stemmed_words1)\n",
    "\n",
    "stemmed_words2 = [ps.stem(word) for word in filtered_words2]\n",
    "print(\"Stemmed Words from text 2:\", stemmed_words2)\n",
    "\n",
    "stemmed_words3 = [ps.stem(word) for word in filtered_words3]\n",
    "print(\"Stemmed Words from text 3:\", stemmed_words3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f732c2ee-ce62-4a5b-bd5c-3660b06ca900",
   "metadata": {},
   "source": [
    "### 3.2 Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "15508ed6-7b97-4df1-8833-b7cb683ddbd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lemmatized Words from text 1: ['Climate', 'change', 'one', 'greatest', 'challenge', 'time', '.', 'latest', 'report', ',', 'released', 'January', '10', ',', '2025', ',', 'state', 'global', 'temperature', 'risen', '1.5Â°C', '.', 'Rising', 'sea', 'level', 'expected', 'displace', '200', 'million', 'people', '2050', '.', 'Experts', 'predict', 'cost', 'addressing', 'issue', 'could', 'exceed', '$', '10', 'trillion', 'immediate', 'action', 'â€™', 'taken', '.']\n",
      "Lemmatized Words from text 2: ['OMG', '!', '!', 'ðŸ˜²', 'watched', 'new', 'netflix', 'documentary', '...', \"'s\", 'mind-blowing', '!', '!', '!', '#', 'Bingewatching', '#', 'FutureTech']\n",
      "Lemmatized Words from text 3: ['E', '=', 'mc^2', 'one', 'famous', 'equation', 'physic', '.', 'principle', 'form', 'basis', 'modern', 'theoretical', 'physic', '.']\n"
     ]
    }
   ],
   "source": [
    "# using a vocabulary-based approach to return the base or dictionary form of a word\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "lemmatized_words1 = [lemmatizer.lemmatize(word) for word in filtered_words1]\n",
    "print(\"Lemmatized Words from text 1:\", lemmatized_words1)\n",
    "\n",
    "lemmatized_words2 = [lemmatizer.lemmatize(word) for word in filtered_words2]\n",
    "print(\"Lemmatized Words from text 2:\", lemmatized_words2)\n",
    "\n",
    "lemmatized_words3 = [lemmatizer.lemmatize(word) for word in filtered_words3]\n",
    "print(\"Lemmatized Words from text 3:\", lemmatized_words3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0e21423-7887-4047-8b0e-0baa6ce0a34c",
   "metadata": {},
   "source": [
    "## 4. Parts of Speech (POS) Tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a2576795-254a-43dc-8a74-7ab5dc4da5f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "POS Tags from text 1: [('Climate', 'NNP'), ('change', 'NN'), ('is', 'VBZ'), ('one', 'CD'), ('of', 'IN'), ('the', 'DT'), ('greatest', 'JJS'), ('challenges', 'NNS'), ('of', 'IN'), ('our', 'PRP$'), ('time', 'NN'), ('.', '.'), ('The', 'DT'), ('latest', 'JJS'), ('report', 'NN'), (',', ','), ('released', 'VBN'), ('on', 'IN'), ('January', 'NNP'), ('10', 'CD'), (',', ','), ('2025', 'CD'), (',', ','), ('states', 'VBZ'), ('that', 'IN'), ('global', 'JJ'), ('temperatures', 'NNS'), ('have', 'VBP'), ('risen', 'VBN'), ('by', 'IN'), ('1.5Â°C', 'CD'), ('.', '.'), ('Rising', 'VBG'), ('sea', 'NN'), ('levels', 'NNS'), ('are', 'VBP'), ('expected', 'VBN'), ('to', 'TO'), ('displace', 'VB'), ('over', 'IN'), ('200', 'CD'), ('million', 'CD'), ('people', 'NNS'), ('by', 'IN'), ('2050', 'CD'), ('.', '.'), ('Experts', 'NNS'), ('predict', 'VBP'), ('that', 'IN'), ('the', 'DT'), ('cost', 'NN'), ('of', 'IN'), ('addressing', 'VBG'), ('these', 'DT'), ('issues', 'NNS'), ('could', 'MD'), ('exceed', 'VB'), ('$', '$'), ('10', 'CD'), ('trillion', 'CD'), ('if', 'IN'), ('immediate', 'JJ'), ('action', 'NN'), ('isn', 'NN'), ('â€™', 'NNP'), ('t', 'NN'), ('taken', 'VBN'), ('.', '.')]\n",
      "POS Tags from text 2: [('OMG', 'NN'), ('!', '.'), ('!', '.'), ('ðŸ˜²', 'NN'), ('Just', 'NNP'), ('watched', 'VBD'), ('the', 'DT'), ('new', 'JJ'), ('netflix', 'NN'), ('documentary', 'NN'), ('...', ':'), ('it', 'PRP'), (\"'s\", 'VBZ'), ('mind-blowing', 'JJ'), ('!', '.'), ('!', '.'), ('!', '.'), ('#', '#'), ('Bingewatching', 'NNP'), ('#', '#'), ('FutureTech', 'NNP')]\n",
      "POS Tags from text 3: [('E', 'NNP'), ('=', 'NNP'), ('mc^2', 'NN'), ('is', 'VBZ'), ('one', 'CD'), ('of', 'IN'), ('the', 'DT'), ('most', 'RBS'), ('famous', 'JJ'), ('equations', 'NNS'), ('in', 'IN'), ('physics', 'NNS'), ('.', '.'), ('This', 'DT'), ('principle', 'NN'), ('forms', 'VBZ'), ('the', 'DT'), ('basis', 'NN'), ('of', 'IN'), ('modern', 'JJ'), ('theoretical', 'JJ'), ('physics', 'NNS'), ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "# assigns grammatical labels to words helping analyze sentence structure\n",
    "\n",
    "from nltk import pos_tag\n",
    "\n",
    "pos_tags1 = pos_tag(words1)\n",
    "print(\"POS Tags from text 1:\", pos_tags1)\n",
    "\n",
    "pos_tags2 = pos_tag(words2)\n",
    "print(\"POS Tags from text 2:\", pos_tags2)\n",
    "\n",
    "pos_tags3 = pos_tag(words3)\n",
    "print(\"POS Tags from text 3:\", pos_tags3)\n",
    "# DT (Determiner), JJ (Adjective), NN (Noun, singular), VBZ (Verb, 3rd person singular present), IN (Preposition), DT (Determiner), JJ (Adjective)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afdf4541-3914-4722-9875-8b2354eb3a2c",
   "metadata": {},
   "source": [
    "## 5. Named Entity Recognition (NER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d5a13bdd-15cf-47ae-8f1c-1d6f6018cf6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Named Entities: (S\n",
      "  (PERSON Climate/NNP)\n",
      "  change/NN\n",
      "  is/VBZ\n",
      "  one/CD\n",
      "  of/IN\n",
      "  the/DT\n",
      "  greatest/JJS\n",
      "  challenges/NNS\n",
      "  of/IN\n",
      "  our/PRP$\n",
      "  time/NN\n",
      "  ./.\n",
      "  The/DT\n",
      "  latest/JJS\n",
      "  report/NN\n",
      "  ,/,\n",
      "  released/VBN\n",
      "  on/IN\n",
      "  January/NNP\n",
      "  10/CD\n",
      "  ,/,\n",
      "  2025/CD\n",
      "  ,/,\n",
      "  states/VBZ\n",
      "  that/IN\n",
      "  global/JJ\n",
      "  temperatures/NNS\n",
      "  have/VBP\n",
      "  risen/VBN\n",
      "  by/IN\n",
      "  1.5Â°C/CD\n",
      "  ./.\n",
      "  Rising/VBG\n",
      "  sea/NN\n",
      "  levels/NNS\n",
      "  are/VBP\n",
      "  expected/VBN\n",
      "  to/TO\n",
      "  displace/VB\n",
      "  over/IN\n",
      "  200/CD\n",
      "  million/CD\n",
      "  people/NNS\n",
      "  by/IN\n",
      "  2050/CD\n",
      "  ./.\n",
      "  Experts/NNS\n",
      "  predict/VBP\n",
      "  that/IN\n",
      "  the/DT\n",
      "  cost/NN\n",
      "  of/IN\n",
      "  addressing/VBG\n",
      "  these/DT\n",
      "  issues/NNS\n",
      "  could/MD\n",
      "  exceed/VB\n",
      "  $/$\n",
      "  10/CD\n",
      "  trillion/CD\n",
      "  if/IN\n",
      "  immediate/JJ\n",
      "  action/NN\n",
      "  isn/NN\n",
      "  â€™/NNP\n",
      "  t/NN\n",
      "  taken/VBN\n",
      "  ./.)\n"
     ]
    }
   ],
   "source": [
    "# identifies specific entities such as names, dates and locations in a text.\n",
    "# NER is often used in tasks like extracting information from documents or creating chatbots.\n",
    "\n",
    "from nltk import ne_chunk\n",
    "entities = ne_chunk(pos_tags1)\n",
    "print(\"Named Entities:\", entities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "76f648cc-ba23-4e78-b6f9-719912d9e88c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# S : Represents Sentence, PERSON - NNP : Proper Nouns, VBZ : Verbs, IN : preposition\n",
    "# GPE : Geopolitical Entity (location) , ./. : end of the sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd7743dd-887f-4b29-94d6-bf26e0fb37c9",
   "metadata": {},
   "source": [
    "## 6. Frequency Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b0aafa79-b9b5-4ff7-8404-462acec4911f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most Common Words in text1: [('.', 4), (',', 3), ('10', 2), ('Climate', 1), ('change', 1), ('one', 1)]\n",
      "Most Common Words in text1: [('.', 4), ('of', 3), (',', 3), ('the', 2), ('10', 2), ('that', 2)]\n"
     ]
    }
   ],
   "source": [
    "# Analyze the frequency of words or tokens to understand text patterns.\n",
    "\n",
    "from nltk.probability import FreqDist\n",
    "fdist_after = FreqDist(filtered_words1)\n",
    "print(\"Most Common Words in text1:\", fdist_after.most_common(6))\n",
    "\n",
    "from nltk.probability import FreqDist\n",
    "fdist_before = FreqDist(words1)\n",
    "print(\"Most Common Words in text1:\", fdist_before.most_common(6))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b55d87c-f445-47e5-8c88-a11bb7a1e93c",
   "metadata": {},
   "source": [
    "## 7. Synonyms and Antonyms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b521c899-2470-4878-b73b-0ae605a19e71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No synonyms found.\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import wordnet \n",
    "\n",
    "synonyms = wordnet.synsets('mc^2')\n",
    "\n",
    "# Check if synonyms were found and print them\n",
    "if synonyms:\n",
    "    for syn in synonyms:\n",
    "        print(f\"Word: {syn.name()}, Definition: {syn.definition()}\")\n",
    "else:\n",
    "    print(\"No synonyms found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9885bc52-9393-4f0e-8388-78d0668bd8cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Antonyms of 'new': ['old', 'worn']\n"
     ]
    }
   ],
   "source": [
    "# Example word\n",
    "word = \"new\"\n",
    "\n",
    "# Find antonyms\n",
    "antonyms = []\n",
    "for syn in wordnet.synsets(word):\n",
    "    for lemma in syn.lemmas():\n",
    "        if lemma.antonyms():\n",
    "            antonyms.append(lemma.antonyms()[0].name())\n",
    "\n",
    "# Display results\n",
    "if antonyms:\n",
    "    print(f\"Antonyms of '{word}': {antonyms}\")\n",
    "else:\n",
    "    print(f\"No antonyms found for '{word}'.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
