{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1f39cbb2-bb8d-4d2a-8c37-3c5e8c4aadd7",
   "metadata": {},
   "source": [
    "# Sentiment Analysis using VADER and Huggingface"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36fe389d-cea9-415e-ab78-9bf9fb4aaa5e",
   "metadata": {},
   "source": [
    "• VADER (Valence Aware Dictionary and Sentiment Reasoner): A lexicon and rule-based sentiment analysis tool that's particularly good at analyzing social media text.           \n",
    "• Huggingface Transformers: Using pre-trained models such as BERT or DistilBERT for sentiment analysis, which is useful for understanding deeper contexts in sentences. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fa42522-bfdb-4522-aacc-686bc46480b4",
   "metadata": {},
   "source": [
    "### Exercise 1: Sentiment Analysis using VADER "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "20bcfe18-00b9-4d57-9b4b-d980260e0b88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing necessary libraries\n",
    "import nltk\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "650d0506-f422-45b1-a3dc-aed0c224481f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializing the VADER SentimentIntensityAnalyzer \n",
    "analyzer = SentimentIntensityAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2ec5ca6b-e169-4127-b011-8fb132903d5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the sample sentences to analyze\n",
    "sentences = [\n",
    "    \"I am so happy with the service.\",\n",
    "    \"This movie was a waste of time.\",\n",
    "    \"It was an okay experience.\",\n",
    "    \"Best purchase I've made in years!\",\n",
    "    \"I don't like this app, it's too slow.\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e8bced9a-dcf5-4783-a394-dd6a77fc304c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: I am so happy with the service.\n",
      "Sentiment Scores: {'neg': 0.0, 'neu': 0.559, 'pos': 0.441, 'compound': 0.6948}\n",
      "Sentiment: Positive\n",
      "\n",
      "\n",
      "Text: This movie was a waste of time.\n",
      "Sentiment Scores: {'neg': 0.318, 'neu': 0.682, 'pos': 0.0, 'compound': -0.4215}\n",
      "Sentiment: Negative\n",
      "\n",
      "\n",
      "Text: It was an okay experience.\n",
      "Sentiment Scores: {'neg': 0.0, 'neu': 0.678, 'pos': 0.322, 'compound': 0.2263}\n",
      "Sentiment: Positive\n",
      "\n",
      "\n",
      "Text: Best purchase I've made in years!\n",
      "Sentiment Scores: {'neg': 0.0, 'neu': 0.527, 'pos': 0.473, 'compound': 0.6696}\n",
      "Sentiment: Positive\n",
      "\n",
      "\n",
      "Text: I don't like this app, it's too slow.\n",
      "Sentiment Scores: {'neg': 0.232, 'neu': 0.768, 'pos': 0.0, 'compound': -0.2755}\n",
      "Sentiment: Negative\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Analyzing sentiment \n",
    "for sentence in sentences: \n",
    "    score = analyzer.polarity_scores(sentence) \n",
    "    print(f\"Text: {sentence}\") \n",
    "    print(f\"Sentiment Scores: {score}\") \n",
    "    print(f\"Sentiment: {'Positive' if score['compound'] >= 0.05 else 'Negative' if score['compound'] <= -0.05 else 'Neutral'}\") \n",
    "    print(\"\\n\") "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2e1e62a-773e-4ab4-b469-693cfa50a254",
   "metadata": {},
   "source": [
    "#### 1. What do the sentiment scores (positive, neutral, negative, and compound) represent? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3b33d30-4621-4dd6-8e5f-869f7ef356f0",
   "metadata": {},
   "source": [
    "The sentiment scores represent the following:  Positive Score – The proportion of positive sentiment in the text.                 \n",
    "Neutral Score – The proportion of neutral sentiment in the text.                                                    \n",
    "Negative Score – The proportion of negative sentiment in the text.                                     \n",
    "Compound Score – A normalized score that sums up the sentiment, ranging from -1 (most negative) to +1 (most positive)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33f218ea-95bb-4a61-ba06-9942cca8e07d",
   "metadata": {},
   "source": [
    "#### 2. How can you classify a sentence as positive, negative, or neutral based on the compound score? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51d3957b-3056-4d58-bdd6-d94acdeb778b",
   "metadata": {},
   "source": [
    "A sentence can be classified based on the compound score from VADER as follows: Positive: If the compound score is ≥ 0.05, the sentence is classified as positive.         \n",
    "Negative: If the compound score is ≤ -0.05, the sentence is classified as negative.                                \n",
    "Neutral: If the compound score is between -0.05 and 0.05, the sentence is classified as neutral."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2402cd3-f245-4ee1-9dd8-cdac28b753c9",
   "metadata": {},
   "source": [
    "### Exercise 2: Sentiment Analysis Using Huggingface Transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "75c3fbc6-60b8-4f05-b375-a72fb9cefc1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "75575023-7eda-4426-a54d-7caf0eaffb38",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert/distilbert-base-uncased-finetuned-sst-2-english and revision 714eb0f (https://huggingface.co/distilbert/distilbert-base-uncased-finetuned-sst-2-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "Device set to use cpu\n"
     ]
    }
   ],
   "source": [
    "# Load the sentiment analysis pipeline from Huggingface \n",
    "sentiment_pipeline = pipeline(\"sentiment-analysis\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "26488bb2-1dff-44e6-a8bf-402eb4e063df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the sample sentences to analyze\n",
    "sentences = [\n",
    "    \"I love this new phone.\",\n",
    "    \"I had a terrible experience with customer support.\",\n",
    "    \"The movie was not bad, but not great either.\",\n",
    "    \"Absolutely loved the restaurant!\",\n",
    "    \"The product arrived damaged, very disappointed.\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7e3ba967-7dba-4553-878d-c72e20e8e010",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence: I am so happy with the service.\n",
      "Sentiment Label: POSITIVE, Confidence Score: 0.9999\n",
      "\n",
      "\n",
      "Sentence: This movie was a waste of time.\n",
      "Sentiment Label: NEGATIVE, Confidence Score: 0.9998\n",
      "\n",
      "\n",
      "Sentence: It was an okay experience.\n",
      "Sentiment Label: POSITIVE, Confidence Score: 0.9998\n",
      "\n",
      "\n",
      "Sentence: Best purchase I've made in years!\n",
      "Sentiment Label: POSITIVE, Confidence Score: 0.9997\n",
      "\n",
      "\n",
      "Sentence: I don't like this app, it's too slow.\n",
      "Sentiment Label: NEGATIVE, Confidence Score: 0.9992\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Analyzing sentiment using Huggingface model \n",
    "for sentence in sentences: \n",
    "    result = sentiment_pipeline(sentence)[0] \n",
    "    print(f\"Sentence: {sentence}\") \n",
    "    print(f\"Sentiment Label: {result['label']}, Confidence Score: {result['score']:.4f}\") \n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db3a1605-4cc2-461b-9401-8c2e1e29e773",
   "metadata": {},
   "source": [
    "#### 1. What are the labels provided by the Huggingface model for sentiment analysis?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a724ccd9-58a4-420b-abb6-0583e9eeb4ce",
   "metadata": {},
   "source": [
    "The Huggingface sentiment analysis model typically provides two labels: \"POSITIVE\" – Indicates that the sentiment is positive.    \n",
    "\"NEGATIVE\" – Indicates that the sentiment is negative."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d12bfeb-cc08-46d0-940c-bc07631aa06b",
   "metadata": {},
   "source": [
    "#### 2. How do the confidence scores relate to the model's prediction? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4f1388e-f550-4f9b-8799-dec093c707c5",
   "metadata": {},
   "source": [
    "The confidence score (ranging from 0 to 1) represents the model’s certainty in its prediction.\n",
    "A higher score means the model is more confident in classifying the sentiment as positive or negative.\n",
    "For example, if the model outputs \"POSITIVE\" with a score of 0.98, it means it is 98% confident that the text conveys positive sentiment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b763285-dd16-4d69-8664-85de8f98458d",
   "metadata": {},
   "source": [
    "### Exercise 3: Compare VADER and Huggingface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "43103476-d8dd-43ba-9c89-7c0ca3bebb12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence: I am so happy with the service.\n",
      "Huggingface Sentiment: POSITIVE, Confidence Score: 0.9999\n",
      "VADER Sentiment: Positive, Compound Score: 0.6948\n",
      "\n",
      "Sentence: This movie was a waste of time.\n",
      "Huggingface Sentiment: NEGATIVE, Confidence Score: 0.9998\n",
      "VADER Sentiment: Negative, Compound Score: -0.4215\n",
      "\n",
      "Sentence: It was an okay experience.\n",
      "Huggingface Sentiment: POSITIVE, Confidence Score: 0.9998\n",
      "VADER Sentiment: Positive, Compound Score: 0.2263\n",
      "\n",
      "Sentence: Best purchase I've made in years!\n",
      "Huggingface Sentiment: POSITIVE, Confidence Score: 0.9997\n",
      "VADER Sentiment: Positive, Compound Score: 0.6696\n",
      "\n",
      "Sentence: I don't like this app, it's too slow.\n",
      "Huggingface Sentiment: NEGATIVE, Confidence Score: 0.9992\n",
      "VADER Sentiment: Negative, Compound Score: -0.2755\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Analyzing and Comparing performance of VADER and Huggingface on the same set of text data\n",
    "for sentence in sentences:\n",
    "    hf_result = sentiment_pipeline(sentence)[0]\n",
    "    vader_result = analyzer.polarity_scores(sentence)\n",
    "    \n",
    "    print(f\"Sentence: {sentence}\")\n",
    "    print(f\"Huggingface Sentiment: {hf_result['label']}, Confidence Score: {hf_result['score']:.4f}\")\n",
    "    print(f\"VADER Sentiment: {'Positive' if vader_result['compound'] >= 0.05 else 'Negative' if vader_result['compound'] <= -0.05 else 'Neutral'}, Compound Score: {vader_result['compound']:.4f}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4a19cce-c629-4764-b684-35b62a638dba",
   "metadata": {},
   "source": [
    "#### 1. How do the results of VADER and Huggingface compare in terms of sentiment classification? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2afb092-0cfb-4cf9-bc19-ff98046aca09",
   "metadata": {},
   "source": [
    "VADER is rule-based and relies on a predefined lexicon, making it better suited for short, informal texts like social media posts. Huggingface Transformers use deep learning, making them more accurate for understanding context and complex sentence structures. In general, both methods agree on simple, clearly positive or negative sentences. However, Huggingface tends to be more reliable for nuanced language."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31333a47-8880-4dc4-a39e-986198aba24e",
   "metadata": {},
   "source": [
    "#### 2. Which method provides a more accurate prediction for complex sentences (e.g., sentences with sarcasm)?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b37f7c97-15f7-49a6-a6d7-37876525c8ad",
   "metadata": {},
   "source": [
    "Huggingface provides more accurate predictions for complex sentences, especially those with sarcasm or subtle sentiment shifts. VADER struggles with sarcasm because it mainly relies on word-based sentiment scores and lacks deep contextual understanding.\n",
    "\n",
    "Example: For a sarcastic sentence like \"Oh great, another Monday!\", VADER may classify it as positive due to the word \"great,\" while Huggingface is more likely to detect the sarcasm correctly."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "222afaf7-8fad-4ad0-9423-5d889963cedb",
   "metadata": {},
   "source": [
    "#### 3. Which method is faster? Why might that be the case?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66b4c9be-bac2-4af7-b42c-3c17386aa4b6",
   "metadata": {},
   "source": [
    "VADER is faster than Huggingface because it is a rule-based approach that uses a simple lexicon and predefined scoring rules to analyze sentiment. It runs directly on CPU without requiring heavy model inference.\n",
    "Huggingface Transformers are slower because they use deep learning models, which require more computational power and time to process each sentence. It involves floating-point operations and matrix multiplications, which are computationally expensive.\n",
    "\n",
    "If speed is the priority, VADER is the better choice. If accuracy and contextual understanding are important, Huggingface Transformers are more reliable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4adef22-dfc5-4482-983e-a3f5ace83e5a",
   "metadata": {},
   "source": [
    "### Exercise 4: Evaluating Sentiment Analysis Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "015bd6a4-bc52-4bb6-8fca-55afe3d5dcc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "16e4b23f-b8bf-49eb-95b4-14a56b67475e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Create a Test Dataset\n",
    "data = {\n",
    "    \"Sentence\": [\n",
    "        \"I love this new phone!\", \"I had a terrible experience with customer support.\", \n",
    "        \"The movie was not bad, but not great either.\", \"Absolutely loved the restaurant!\", \n",
    "        \"The product arrived damaged, very disappointed.\", \"This is the best purchase I've made!\", \n",
    "        \"I don't think I will buy this again.\", \"Service was okay, nothing special.\",\n",
    "        \"The app crashes frequently, very frustrating.\", \"I am very satisfied with the service.\"\n",
    "    ],\n",
    "    \"True Sentiment\": [\"positive\", \"negative\", \"neutral\", \"positive\", \"negative\", \n",
    "                        \"positive\", \"negative\", \"neutral\", \"negative\", \"positive\"]\n",
    "}\n",
    "df = pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "71a8171c-9991-4dc7-b03a-4dedce8f05f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert/distilbert-base-uncased-finetuned-sst-2-english and revision 714eb0f (https://huggingface.co/distilbert/distilbert-base-uncased-finetuned-sst-2-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence: I love this new phone!\n",
      "Huggingface Sentiment: positive, Confidence Score: 0.9998\n",
      "VADER Sentiment: positive, Compound Score: 0.6696\n",
      "\n",
      "Sentence: I had a terrible experience with customer support.\n",
      "Huggingface Sentiment: negative, Confidence Score: 0.9995\n",
      "VADER Sentiment: negative, Compound Score: -0.1027\n",
      "\n",
      "Sentence: The movie was not bad, but not great either.\n",
      "Huggingface Sentiment: negative, Confidence Score: 0.9963\n",
      "VADER Sentiment: negative, Compound Score: -0.5448\n",
      "\n",
      "Sentence: Absolutely loved the restaurant!\n",
      "Huggingface Sentiment: positive, Confidence Score: 0.9999\n",
      "VADER Sentiment: positive, Compound Score: 0.6689\n",
      "\n",
      "Sentence: The product arrived damaged, very disappointed.\n",
      "Huggingface Sentiment: negative, Confidence Score: 0.9998\n",
      "VADER Sentiment: negative, Compound Score: -0.7425\n",
      "\n",
      "Sentence: This is the best purchase I've made!\n",
      "Huggingface Sentiment: positive, Confidence Score: 0.9999\n",
      "VADER Sentiment: positive, Compound Score: 0.6696\n",
      "\n",
      "Sentence: I don't think I will buy this again.\n",
      "Huggingface Sentiment: negative, Confidence Score: 0.9996\n",
      "VADER Sentiment: neutral, Compound Score: 0.0000\n",
      "\n",
      "Sentence: Service was okay, nothing special.\n",
      "Huggingface Sentiment: negative, Confidence Score: 0.9868\n",
      "VADER Sentiment: negative, Compound Score: -0.0920\n",
      "\n",
      "Sentence: The app crashes frequently, very frustrating.\n",
      "Huggingface Sentiment: negative, Confidence Score: 0.9997\n",
      "VADER Sentiment: negative, Compound Score: -0.4927\n",
      "\n",
      "Sentence: I am very satisfied with the service.\n",
      "Huggingface Sentiment: positive, Confidence Score: 0.9998\n",
      "VADER Sentiment: positive, Compound Score: 0.4754\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Step 2: Perform Sentiment Analysis\n",
    "# Initialize Sentiment Analyzers\n",
    "analyzer = SentimentIntensityAnalyzer()\n",
    "sentiment_pipeline = pipeline(\"sentiment-analysis\")\n",
    "\n",
    "vader_predictions = []\n",
    "huggingface_predictions = []\n",
    "\n",
    "for sentence in df[\"Sentence\"]:\n",
    "    hf_result = sentiment_pipeline(sentence)[0]\n",
    "    vader_result = analyzer.polarity_scores(sentence)\n",
    "    \n",
    "    hf_sentiment = \"positive\" if hf_result[\"label\"] == \"POSITIVE\" else \"negative\"\n",
    "    vader_sentiment = \"positive\" if vader_result[\"compound\"] >= 0.05 else \"negative\" if vader_result[\"compound\"] <= -0.05 else \"neutral\"\n",
    "    \n",
    "    vader_predictions.append(vader_sentiment)\n",
    "    huggingface_predictions.append(hf_sentiment)\n",
    "    \n",
    "    print(f\"Sentence: {sentence}\")\n",
    "    print(f\"Huggingface Sentiment: {hf_sentiment}, Confidence Score: {hf_result['score']:.4f}\")\n",
    "    print(f\"VADER Sentiment: {vader_sentiment}, Compound Score: {vader_result['compound']:.4f}\\n\")\n",
    "\n",
    "# Store predictions in DataFrame\n",
    "df[\"VADER Prediction\"] = vader_predictions\n",
    "df[\"Huggingface Prediction\"] = huggingface_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "0f1f646d-f6de-4b20-93cb-4cdb9c3ec12a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hp\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "# Step 3: Calculate Evaluation Metrics\n",
    "label_mapping = {\"positive\": 1, \"neutral\": 0, \"negative\": -1}\n",
    "df[\"True Sentiment Num\"] = df[\"True Sentiment\"].map(label_mapping)\n",
    "df[\"VADER Prediction Num\"] = df[\"VADER Prediction\"].map(label_mapping)\n",
    "df[\"Huggingface Prediction Num\"] = df[\"Huggingface Prediction\"].map(label_mapping)\n",
    "\n",
    "# Evaluate both models\n",
    "vader_metrics = [\n",
    "    accuracy_score(df[\"True Sentiment Num\"], df[\"VADER Prediction Num\"]),\n",
    "    precision_score(df[\"True Sentiment Num\"], df[\"VADER Prediction Num\"], average=\"macro\"),\n",
    "    recall_score(df[\"True Sentiment Num\"], df[\"VADER Prediction Num\"], average=\"macro\"),\n",
    "    f1_score(df[\"True Sentiment Num\"], df[\"VADER Prediction Num\"], average=\"macro\")\n",
    "]\n",
    "\n",
    "huggingface_metrics = [\n",
    "    accuracy_score(df[\"True Sentiment Num\"], df[\"Huggingface Prediction Num\"]),\n",
    "    precision_score(df[\"True Sentiment Num\"], df[\"Huggingface Prediction Num\"], average=\"macro\"),\n",
    "    recall_score(df[\"True Sentiment Num\"], df[\"Huggingface Prediction Num\"], average=\"macro\"),\n",
    "    f1_score(df[\"True Sentiment Num\"], df[\"Huggingface Prediction Num\"], average=\"macro\")\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "ee513820-c4ab-4477-af2b-d77b733007fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VADER Performance:\n",
      "Accuracy: 0.70, Precision: 0.53, Recall: 0.58, F1-Score: 0.56\n",
      "\n",
      "Huggingface Performance:\n",
      "Accuracy: 0.80, Precision: 0.56, Recall: 0.67, F1-Score: 0.60\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Print Results\n",
    "print(\"VADER Performance:\")\n",
    "print(f\"Accuracy: {vader_metrics[0]:.2f}, Precision: {vader_metrics[1]:.2f}, Recall: {vader_metrics[2]:.2f}, F1-Score: {vader_metrics[3]:.2f}\\n\")\n",
    "\n",
    "print(\"Huggingface Performance:\")\n",
    "print(f\"Accuracy: {huggingface_metrics[0]:.2f}, Precision: {huggingface_metrics[1]:.2f}, Recall: {huggingface_metrics[2]:.2f}, F1-Score: {huggingface_metrics[3]:.2f}\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8bdc71a-b4a7-4d6d-8482-2d0b7fecfd9e",
   "metadata": {},
   "source": [
    "#### 1. How do the models perform in terms of accuracy, precision, recall, and F1-score? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65d3e217-cd62-4562-967c-399b1900fc80",
   "metadata": {},
   "source": [
    "Accuracy: Huggingface outperforms VADER in terms of overall accuracy, with a significant margin of 0.10 (80% vs. 70%).\n",
    "\n",
    "Precision: Huggingface has a slightly better precision (0.56 vs. 0.53), which means it performs marginally better at identifying positive or negative sentiments when it labels them.\n",
    "\n",
    "Recall: Huggingface again outperforms VADER in recall (0.67 vs. 0.58), indicating it is better at identifying the true positives or negatives.\n",
    "\n",
    "F1-Score: Huggingface has a higher F1-score (0.60 vs. 0.56), indicating a more balanced performance between precision and recall."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "743e397d-303d-451c-a770-d55162709a5b",
   "metadata": {},
   "source": [
    "#### 2. Which model performs better in predicting positive sentiment? Negative sentiment?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9891ef43-6061-430b-9488-fc4245ca9f68",
   "metadata": {},
   "source": [
    "Positive Sentiment: Both models predict positive sentiment with high confidence. However, Huggingface seems to perform better when you look at the overall metrics, particularly the recall, which is better for Huggingface (0.67 vs. 0.58). This implies Huggingface does a better job of identifying positive sentiment instances.\n",
    "\n",
    "Negative Sentiment: Huggingface again performs better for negative sentiment predictions based on the recall (0.67 vs. 0.58) and overall accuracy. Huggingface has higher recall, meaning it correctly identifies more instances of negative sentiment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9aa2827-7155-43b0-a661-e13d201c9dc3",
   "metadata": {},
   "source": [
    "#### 3. What might cause discrepancies between the two models' predictions?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f7664a2-f9d7-4cd8-b8c1-b0ecfab9dc5b",
   "metadata": {},
   "source": [
    "Several factors could explain the discrepancies in predictions between VADER and Huggingface:\n",
    "\n",
    "Model Type: VADER is a rule-based sentiment analysis tool that relies on lexicons and predefined rules, while Huggingface uses a machine learning model (likely fine-tuned on a specific dataset) that learns from large-scale data to classify sentiment. This difference in approach can result in variations in how each model interprets sentiment.\n",
    "\n",
    "Training Data: Huggingface likely uses a more diverse and larger dataset for training, which might allow it to capture more nuances in sentiment, whereas VADER relies on a fixed set of rules and lexicons. This can lead to better handling of complex or ambiguous sentences by Huggingface.\n",
    "\n",
    "Context Sensitivity: Huggingface, being a machine learning model, might be more sensitive to context and subtleties in phrasing. For example, in the sentence \"I don't think I will buy this again,\" VADER labels it as neutral, while Huggingface predicts it as negative. Huggingface may better capture the implied sentiment, whereas VADER's rule-based approach struggles with such subtleties.\n",
    "\n",
    "Thresholds for Classification: VADER uses a compound score that can be mapped to sentiment (positive, negative, neutral), and sometimes the threshold for deciding the sentiment can be more conservative or less nuanced compared to the confidence score-based approach of Huggingface, which might lead to differing sentiment labels for borderline cases.\n",
    "\n",
    "Confidence Scores: Huggingface provides a confidence score (e.g., 0.9998), which may offer more clarity on how sure the model is about the classification. VADER doesn’t provide a confidence score, which might result in less informative outputs when comparing the models directly."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
