{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "500fcef9-9d59-45f1-8cb8-39f816893723",
   "metadata": {},
   "source": [
    "# Text Analysis using TF-IDF, Bag of Words and N-Grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1f073c94-aeab-4fbd-9200-f6306e7db384",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TF-IDF a statistical measure used in information retrieval and text mining.\n",
    "# It helps to evaluate the importance of a word in a document relative to a collection of documents (corpus). \n",
    "\n",
    "# Applications of TF-IDF \n",
    "# • Text classification \n",
    "# • Keyword extraction\n",
    "# • Information retrieval and search engines\n",
    "# • Text clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "560c451c-a212-4406-a9d9-445ae4bc7aed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bag of Words (BoW) \n",
    "# A representation of text data where each document is represented by word frequencies. \n",
    "# It converts each document into a vector based on the frequency of words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "fefb597e-0b90-4bce-8897-a37162080f8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# N-Grams\n",
    "# N-Grams are contiguous sequences of n words from a given text. For example: \n",
    "# 1-Grams: Single words (unigrams). \n",
    "# 2-Grams: Pairs of words (bigrams). \n",
    "# 3-Grams: Triples of words (trigrams)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd2e91dc-ab35-4508-856e-efc195168b97",
   "metadata": {},
   "source": [
    "## STEP 1: Set up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "567ab775-4b20-40cd-b2f2-032b623f6227",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the necessary libraries and module\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7822e71b-07f6-4241-9313-86ebf1635865",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the sample dataset into a list\n",
    "documents = [ \n",
    "\"Data science is an interdisciplinary field.\", \n",
    "\"Machine learning is a subset of artificial intelligence.\", \n",
    "\"Data science uses machine learning algorithms.\", \n",
    "\"Artificial intelligence and data science are growing fields.\" \n",
    "] "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c079a012-0d95-42b0-9f8f-e826955d9b3b",
   "metadata": {},
   "source": [
    "## STEP 2: Bag of Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a52b4fb6-14cf-475f-95ca-f636bbf22056",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bag of Words Model:\n",
      "    algorithms  an  and  are  artificial  data  field  fields  growing  \\\n",
      "0           0   1    0    0           0     1      1       0        0   \n",
      "1           0   0    0    0           1     0      0       0        0   \n",
      "2           1   0    0    0           0     1      0       0        0   \n",
      "3           0   0    1    1           1     1      0       1        1   \n",
      "\n",
      "   intelligence  interdisciplinary  is  learning  machine  of  science  \\\n",
      "0             0                  1   1         0        0   0        1   \n",
      "1             1                  0   1         1        1   1        0   \n",
      "2             0                  0   0         1        1   0        1   \n",
      "3             1                  0   0         0        0   0        1   \n",
      "\n",
      "   subset  uses  \n",
      "0       0     0  \n",
      "1       1     0  \n",
      "2       0     1  \n",
      "3       0     0  \n"
     ]
    }
   ],
   "source": [
    "# Initializing CountVectorizer\n",
    "vectorizer_bow = CountVectorizer()\n",
    "\n",
    "# Transforming into a corpus\n",
    "X_bow = vectorizer_bow.fit_transform(documents)\n",
    "\n",
    "# Converting the result into a DataFrame for readability\n",
    "bow_df = pd.DataFrame(X_bow.toarray(), columns=vectorizer_bow.get_feature_names_out())\n",
    "print(\"Bag of Words Model:\\n\", bow_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c72c066b-5a0a-4e72-9d55-da50c39feee5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explanation: \n",
    "# Each row represents a document. \n",
    "# Each column represents a word from the corpus. \n",
    "# The values in the matrix represent the frequency of the words in the corresponding document."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "214c28b3-7272-42f6-b11e-5256ffe41d1a",
   "metadata": {},
   "source": [
    "#### 1. Which word appears most frequently across all documents?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "db65791c-a68b-4312-93fd-300f7995b1d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Most frequent words: ['data', 'science']\n"
     ]
    }
   ],
   "source": [
    "word_frequencies = bow_df.sum()\n",
    "most_frequent_words = word_frequencies[word_frequencies == word_frequencies.max()].index.tolist()\n",
    "print(f\"==> Most frequent words: {most_frequent_words}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "771b6bbc-7bd9-422b-b0cb-0211750d97dd",
   "metadata": {},
   "source": [
    "#### 2. Are there any words that appear only once?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "fc75f57b-5dd1-47d6-86a8-cf8d770ab289",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Words that appear only once: ['algorithms', 'an', 'and', 'are', 'field', 'fields', 'growing', 'interdisciplinary', 'of', 'subset', 'uses']\n"
     ]
    }
   ],
   "source": [
    "unique_words = bow_df.columns[bow_df.sum() == 1].tolist()\n",
    "print(f\"==> Words that appear only once: {unique_words}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9efb7a41-3a10-4665-8876-44792c8f617d",
   "metadata": {},
   "source": [
    "## STEP 3: TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "3b44bd78-83fb-4df0-ae0f-f9d1179bb52c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF Model:\n",
      "    algorithms  artificial      data     field    fields   growing  \\\n",
      "0    0.000000    0.000000  0.380444  0.596039  0.000000  0.000000   \n",
      "1    0.000000    0.422247  0.000000  0.000000  0.000000  0.000000   \n",
      "2    0.496414    0.000000  0.316854  0.000000  0.000000  0.000000   \n",
      "3    0.000000    0.391378  0.316854  0.000000  0.496414  0.496414   \n",
      "\n",
      "   intelligence  interdisciplinary  learning   machine   science    subset  \\\n",
      "0      0.000000           0.596039  0.000000  0.000000  0.380444  0.000000   \n",
      "1      0.422247           0.000000  0.422247  0.422247  0.000000  0.535566   \n",
      "2      0.000000           0.000000  0.391378  0.391378  0.316854  0.000000   \n",
      "3      0.391378           0.000000  0.000000  0.000000  0.316854  0.000000   \n",
      "\n",
      "       uses  \n",
      "0  0.000000  \n",
      "1  0.000000  \n",
      "2  0.496414  \n",
      "3  0.000000  \n"
     ]
    }
   ],
   "source": [
    "# Initializing the vectorizer (removing common English stop words)\n",
    "vectorizer_tfidf = TfidfVectorizer(stop_words = 'english')\n",
    "\n",
    "# Transforming into a corpus\n",
    "X_tfidf = vectorizer_tfidf.fit_transform(documents)\n",
    "\n",
    "# Converting the result into a DataFrame for readability\n",
    "tfidf_df = pd.DataFrame(X_tfidf.toarray(), columns=vectorizer_tfidf.get_feature_names_out())\n",
    "print(\"TF-IDF Model:\\n\", tfidf_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "294b01ac-68b9-4b6d-9b19-a8cc2b568e76",
   "metadata": {},
   "source": [
    "#### 3. Which term has the highest TF-IDF score in the first document?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "c244f950-57fd-4e9d-9642-b57a0d148d24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> In first document, term with highest TF-IDF score: field \n",
      " Score: 0.5960389368177127\n"
     ]
    }
   ],
   "source": [
    "highest_tfidf_term = tfidf_df.iloc[0].idxmax()\n",
    "highest_tfidf_score = tfidf_df.iloc[0].max()\n",
    "print(f\"==> In first document, term with highest TF-IDF score: {highest_tfidf_term} \\n Score: {highest_tfidf_score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "582750bf-fdb7-42db-8a7e-d50d08ec98c9",
   "metadata": {},
   "source": [
    "#### 4. Why do some terms have a TF-IDF score of 0 in certain documents? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d3e7539-6bed-47f4-922b-84b230536c72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A term has a TF-IDF score of 0 in a document means that the term is not important. \n",
    "# This happens when :\n",
    "# a. The term does not appear in the document (TF = 0)\n",
    "# b. The term appears in all documents (IDF = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ab11a3f-6cfc-4288-9615-67553e374f03",
   "metadata": {},
   "source": [
    "## STEP 4: N-Grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "038dbca0-f33f-4a0e-823a-70a9409f31af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bigrams Model:\n",
      "    an interdisciplinary  and data  are growing  artificial intelligence  \\\n",
      "0                     1         0            0                        0   \n",
      "1                     0         0            0                        1   \n",
      "2                     0         0            0                        0   \n",
      "3                     0         1            1                        1   \n",
      "\n",
      "   data science  growing fields  intelligence and  interdisciplinary field  \\\n",
      "0             1               0                 0                        1   \n",
      "1             0               0                 0                        0   \n",
      "2             1               0                 0                        0   \n",
      "3             1               1                 1                        0   \n",
      "\n",
      "   is an  is subset  learning algorithms  learning is  machine learning  \\\n",
      "0      1          0                    0            0                 0   \n",
      "1      0          1                    0            1                 1   \n",
      "2      0          0                    1            0                 1   \n",
      "3      0          0                    0            0                 0   \n",
      "\n",
      "   of artificial  science are  science is  science uses  subset of  \\\n",
      "0              0            0           1             0          0   \n",
      "1              1            0           0             0          1   \n",
      "2              0            0           0             1          0   \n",
      "3              0            1           0             0          0   \n",
      "\n",
      "   uses machine  \n",
      "0             0  \n",
      "1             0  \n",
      "2             1  \n",
      "3             0  \n"
     ]
    }
   ],
   "source": [
    "# To generate N-Grams, you can adjust the ngram_range parameter of CountVectorizer. \n",
    "# Initializing CountVectorizer with bigrams (ngram_range=(2, 2)) \n",
    "vectorizer_ngrams = CountVectorizer(ngram_range=(2, 2))\n",
    "\n",
    "# Transforming into a corpus\n",
    "X_bigrams = vectorizer_ngrams.fit_transform(documents)\n",
    "\n",
    "# Convert the result into a DataFrame for readability\n",
    "bigrams_df = pd.DataFrame(X_bigrams.toarray(), columns=vectorizer_ngrams.get_feature_names_out())\n",
    "print(\"Bigrams Model:\\n\", bigrams_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "1bc6bdf7-5f61-4de2-9d5e-bf1037a1d5c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explanation: \n",
    "# The columns represent the 2-grams (bigrams) that occur in the text. \n",
    "# The values in the matrix indicate the frequency of each bigram in the corresponding document. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b96b8cd6-bf32-4271-a881-c8daf2e0a442",
   "metadata": {},
   "source": [
    "#### 5. Which bigram is most frequent across all documents? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "091b5456-1fe4-4780-af3f-8472a5c2a877",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Most frequent bigram: data science\n"
     ]
    }
   ],
   "source": [
    "most_frequent_bigram = bigrams_df.sum().idxmax()\n",
    "print(f\"==> Most frequent bigram: {most_frequent_bigram}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08ec4335-fc9b-489d-b6de-2864d22e2d2b",
   "metadata": {},
   "source": [
    "#### 6. How might bigrams provide additional context compared to unigrams?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "337c4eda-e6ac-4394-a7a4-94f66fd0af18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bigrams look at two words together whereas unigrams treat each word separately. \n",
    "# This means they capture the order and relationship between words, helping us understand phrases better. \n",
    "# For example:\n",
    "# “New York” specifically refers to the well-known city.\n",
    "# Treating “New” and “York” as separate unigrams might not clearly indicate that meaning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26f6af7b-132f-42a0-b8ef-2628a48cc959",
   "metadata": {},
   "source": [
    "## STEP 5: N-Grams"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb37b6f4-b29f-4cba-bea2-2c77ba315bd1",
   "metadata": {},
   "source": [
    "### Compare the results from BoW and TF-IDF: \n",
    "#### 7. How does TF-IDF handle frequent terms differently from BoW?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "50389e3d-968c-446c-95e2-5868438c3c4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TF-IDF assigns lower weights to words that appear very often across all documents, \n",
    "# while BoW simply counts each word's frequency without considering its commonness across the dataset. \n",
    "# This means that in TF-IDF, common words (even if frequent in a document) are down-weighted, \n",
    "# allowing rarer but potentially more important words to stand out."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dff7dfa4-f916-4fc4-b89c-f53b38bcc083",
   "metadata": {},
   "source": [
    "#### 8. Why might TF-IDF be preferred for some applications?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "e7dfd361-912e-435b-ab1a-369fd7519b31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TF-IDF is often preferred because it doesn't just count words like the bag-of-words model does. \n",
    "# Instead, it gives less importance to very common words (like \"the\" or \"and\") and more importance to words that are unique or rare in a document. \n",
    "# This helps highlight the key words that truly define the content, which is very useful in applications like search engines and document classification."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f67e472-4950-475d-986a-8c6d2d890117",
   "metadata": {},
   "source": [
    "### Compare unigrams (singe words) with bigrams:\n",
    "#### 9. How do bigrams capture relationships between words?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "e5c6dd63-6377-47e3-b97f-0fa37c66e354",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bigrams capture relationships by pairing consecutive words to form meaningful phrases. \n",
    "# For example, in our sample dataset, while unigrams treat \"data\" and \"science\" as separate tokens, \n",
    "# a bigram like \"data science\" preserves the link between the two words, conveying a specific concept. \n",
    "# This ordering helps the model recognize context and associations—like \n",
    "# \"machine learning\" or \"artificial intelligence\" —which unigrams alone might miss."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84712a17-e438-4bbe-8534-88b4365c638b",
   "metadata": {},
   "source": [
    "#### 10. Provide an example where a bigram adds more meaning than individual words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "af43ab1b-a31c-4c8a-ac45-6e93d58ceb9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Consider the bigram \"Social Media.\"\n",
    "# Individually, \"Social\" and \"Media\" each have broad meanings.\n",
    "# But when paired together as a bigram, \n",
    "# \"Social Media\" refers to a specific concept: digital platforms where users create and share content or engage with each other. \n",
    "# Without the bigram, the meaning would be much less clear, as \"Social\" could refer to various social aspects, \n",
    "# \"Media\" could refer to any form of communication."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
