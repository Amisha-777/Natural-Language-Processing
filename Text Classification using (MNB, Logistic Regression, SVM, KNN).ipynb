{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6d8d3579-4287-48bc-9215-c51f7e729f02",
   "metadata": {},
   "source": [
    "## Text Classification Using MNB, Logistic Regression, SVM, and KNN on IMDB Reviews Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e958b19c-7898-4451-9c96-a1b293656449",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text classification is the process of categorizing text into organized groups.\n",
    "# For example, spam filtering, sentiment analysis, and topic labeling are common use cases of text classification."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b9990e1-8ad8-4d73-83d6-fa26395771f7",
   "metadata": {},
   "source": [
    "### Step 1: Importing Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "cc024ddc-47b2-4137-8f7a-f831e3a97275",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package movie_reviews to\n",
      "[nltk_data]     C:\\Users\\hp\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package movie_reviews is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\hp\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\hp\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Data manipulation\n",
    "import pandas as pd\n",
    "import string\n",
    "\n",
    "# Text processing\n",
    "import nltk\n",
    "from nltk.corpus import movie_reviews, stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Feature extraction\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Machine learning models\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# Evaluation metrics\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "# Download necessary NLTK resources\n",
    "nltk.download('movie_reviews')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe473e12-0ca2-4428-8516-01787a8872e2",
   "metadata": {},
   "source": [
    "### Step 2:  Load and Preprocess Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "02e2d10a-b039-4a77-afd7-e60a5efbe42d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                text     label\n",
      "0  plot : two teen couples go to a church party ,...  Negative\n",
      "1  the happy bastard's quick movie review \\ndamn ...  Negative\n",
      "2  it is movies like these that make a jaded movi...  Negative\n",
      "3   \" quest for camelot \" is warner bros . ' firs...  Negative\n",
      "4  synopsis : a mentally unstable man undergoing ...  Negative\n",
      "                                                   text     label\n",
      "1000  films adapted from comic books have had plenty...  Positive\n",
      "1001  every now and then a movie comes along from a ...  Positive\n",
      "1002  you've got mail works alot better than it dese...  Positive\n",
      "1003   \" jaws \" is a rare film that grabs your atten...  Positive\n",
      "1004  moviemaking is a lot like being the general ma...  Positive\n"
     ]
    }
   ],
   "source": [
    "# 1. Load IMDB movie reviews dataset\n",
    "documents = []\n",
    "labels = []\n",
    "\n",
    "for category in movie_reviews.categories():  # \"pos\" and \"neg\"\n",
    "    for fileid in movie_reviews.fileids(category):  # Get review file IDs\n",
    "        text = movie_reviews.raw(fileid)  # Extract review text\n",
    "        documents.append(text)\n",
    "        labels.append('Positive' if category == 'pos' else 'Negative')  # Label as 'pos' or 'neg'\n",
    "\n",
    "# Create a DataFrame\n",
    "df = pd.DataFrame({\"text\": documents, \"label\": labels})\n",
    "\n",
    "# Display the first few rows\n",
    "print(df.head(5))\n",
    "\n",
    "print(df[df[\"label\"] == \"Positive\"].head(5))  # Display first 5 positive reviews\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "f15dd2d3-e74d-48c0-9975-d417d3f31c30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                text     label\n",
      "0  plot two teen couples go church party drink dr...  Negative\n",
      "1  happy bastard 's quick movie review damn y2k b...  Negative\n",
      "2  movies like make jaded movie viewer thankful i...  Negative\n",
      "3  `` quest camelot `` warner bros first feature-...  Negative\n",
      "4  synopsis mentally unstable man undergoing psyc...  Negative\n",
      "                                                   text     label\n",
      "1000  films adapted comic books plenty success wheth...  Positive\n",
      "1001  every movie comes along suspect studio every i...  Positive\n",
      "1002  've got mail works alot better deserves order ...  Positive\n",
      "1003  `` jaws `` rare film grabs attention shows sin...  Positive\n",
      "1004  moviemaking lot like general manager nfl team ...  Positive\n"
     ]
    }
   ],
   "source": [
    "# 2. Preprocessing Text Data\n",
    "\n",
    "# Convert text to lowercase\n",
    "df[\"text\"] = df[\"text\"].str.lower()\n",
    "\n",
    "# Tokenization (Splitting text into words)\n",
    "df[\"text\"] = df[\"text\"].apply(lambda x: word_tokenize(x))\n",
    "\n",
    "# Remove punctuation\n",
    "df[\"text\"] = df[\"text\"].apply(lambda x: [word for word in x if word not in string.punctuation])\n",
    "\n",
    "# Remove stopwords\n",
    "stop_words = set(stopwords.words('english'))  # Get stop words\n",
    "df[\"text\"] = df[\"text\"].apply(lambda x: [word for word in x if word not in stop_words])\n",
    "\n",
    "# Convert tokens back into a sentence\n",
    "df[\"text\"] = df[\"text\"].apply(lambda x: \" \".join(x))\n",
    "\n",
    "# Display the first few rows\n",
    "print(df.head(5))\n",
    "print(df[df[\"label\"] == \"Positive\"].head(5))  # Display first 5 positive reviews"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e7fe671-9f19-4954-9b3c-194a04fa27b3",
   "metadata": {},
   "source": [
    "### Step 3: Convert Text Data into Numerical Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "c2e1fe0a-a22d-488d-97df-de9ae083db5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert text into numerical data using TF-IDF (Term Frequency - Inverse Document Frequency) \n",
    "\n",
    "tfidf = TfidfVectorizer(max_features = 5000) # maximum feature size (e.g., 5000 features) for efficient processing.\n",
    "X = tfidf.fit_transform(df['text'])  # Transform text into numerical form\n",
    "y = df['label']  # Target labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ba35022-a6c4-44dd-8396-457d0a63a6c8",
   "metadata": {},
   "source": [
    "### Step 4: Split Data into Training and Testing Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "4257a3c8-50be-478d-91ce-286d07c69683",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 70% training, 30% testing to evaluate model performance\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state = 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6414681c-6995-4b02-b238-a4f9eb2eae72",
   "metadata": {},
   "source": [
    "### Step 5: Train and Evaluate Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "9a8768b2-5f70-4f08-a102-0c816d54ef5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define models to train\n",
    "models = {\n",
    "    \"Naive Bayes\": MultinomialNB(),\n",
    "    \"Logistic Regression\": LogisticRegression(max_iter = 1000),\n",
    "    \"SVM\": SVC(kernel='linear'),\n",
    "    \"KNN\": KNeighborsClassifier(n_neighbors = 5)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "1e82fa99-081c-40d9-a81c-1e1694a6efc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Naive Bayes...\n",
      "Naive Bayes Accuracy: 0.78\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Negative       0.78      0.79      0.78       308\n",
      "    Positive       0.77      0.76      0.77       292\n",
      "\n",
      "    accuracy                           0.78       600\n",
      "   macro avg       0.78      0.78      0.78       600\n",
      "weighted avg       0.78      0.78      0.78       600\n",
      "\n",
      "--------------------------------------------------\n",
      "Training Logistic Regression...\n",
      "Logistic Regression Accuracy: 0.81\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Negative       0.83      0.78      0.80       308\n",
      "    Positive       0.78      0.84      0.81       292\n",
      "\n",
      "    accuracy                           0.81       600\n",
      "   macro avg       0.81      0.81      0.80       600\n",
      "weighted avg       0.81      0.81      0.80       600\n",
      "\n",
      "--------------------------------------------------\n",
      "Training SVM...\n",
      "SVM Accuracy: 0.82\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Negative       0.84      0.80      0.82       308\n",
      "    Positive       0.80      0.84      0.82       292\n",
      "\n",
      "    accuracy                           0.82       600\n",
      "   macro avg       0.82      0.82      0.82       600\n",
      "weighted avg       0.82      0.82      0.82       600\n",
      "\n",
      "--------------------------------------------------\n",
      "Training KNN...\n",
      "KNN Accuracy: 0.64\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Negative       0.67      0.58      0.62       308\n",
      "    Positive       0.61      0.70      0.65       292\n",
      "\n",
      "    accuracy                           0.64       600\n",
      "   macro avg       0.64      0.64      0.64       600\n",
      "weighted avg       0.64      0.64      0.64       600\n",
      "\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Train and evaluate models\n",
    "accuracies = {}\n",
    "for name, model in models.items():\n",
    "    print(f\"Training {name}...\")\n",
    "    model.fit(X_train, y_train)  # Train the model\n",
    "    y_pred = model.predict(X_test)  # Predict labels for test set\n",
    "    accuracy = accuracy_score(y_test, y_pred)  # Calculate accuracy\n",
    "    accuracies[name] = accuracy  # Store accuracy\n",
    "\n",
    "# Print accuracy and classification report\n",
    "    print(f\"{name} Accuracy: {accuracy:.2f}\")\n",
    "    print(\"Classification Report:\")\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    print(\"-\" * 50)  # Separator for readability"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cdd2e76-f199-4fcb-9476-6b0e07f17e3d",
   "metadata": {},
   "source": [
    "### Step 6: Compare Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "1c3dcb62-4b12-44d9-9c7d-63ae50fea368",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model Comparison:\n",
      "Naive Bayes: 0.78\n",
      "Logistic Regression: 0.81\n",
      "SVM: 0.82\n",
      "KNN: 0.64\n"
     ]
    }
   ],
   "source": [
    "# Step 6: Compare Results\n",
    "# Print final comparison of model accuracies\n",
    "print(\"\\nModel Comparison:\")\n",
    "for name, acc in accuracies.items():\n",
    "    print(f\"{name}: {acc:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8b597ad-c464-4783-b31a-15bd822bd76d",
   "metadata": {},
   "source": [
    "Based on the observed results, the best-performing model is determined by the highest accuracy. Multinomial Naive Bayes (MNB) performed well due to its efficiency in text classification, especially when features are independent. Logistic Regression (LR) was also effective, handling large datasets well, but it struggles with non-linearity. Support Vector Machine (SVM) excelled in high-dimensional spaces but is computationally expensive. K-Nearest Neighbors (KNN) was the least efficient due to its high computational cost and poor scalability for large datasets. Given the accuracy scores, the best-performing model is the most suitable for sentiment analysis in this scenario, providing the best balance between efficiency and accuracy."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
